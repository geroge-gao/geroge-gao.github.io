<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F09%2F24%2Fpandas%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[1234%matplotlib inlineimport numpy as npimport pandas as pdimport matplotlib.pyplot as plt1234# 加载数据data_01 = pd.read_csv('data/data_01.csv')data_02 = pd.read_csv('data/data_02.csv')data_03 = pd.read_csv('data/data_03.csv') 将三个数据合并concat(objs, axis,join) objs: series，dataframe或者pannel构成的list序列 axis: 需要合并链接的轴，0表示的是行，1表示的是列,默认为0 join: 表示连接方式，inner或者outer 123data = pd.concat([data_01, data_02, data_03])data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cand_nm contbr_nm contbr_st contbr_employer contbr_occupation contb_receipt_amt contb_receipt_dt 0 Bachmann, Michelle HARVEY, WILLIAM AL RETIRED RETIRED 250.0 20-Jun-11 1 Bachmann, Michelle HARVEY, WILLIAM AL RETIRED RETIRED 50.0 23-Jun-11 2 Bachmann, Michelle SMITH, LANIER AL INFORMATION REQUESTED INFORMATION REQUESTED 250.0 5-Jul-11 3 Bachmann, Michelle BLEVINS, DARONDA AR NONE RETIRED 250.0 1-Aug-11 4 Bachmann, Michelle WARDENBURG, HAROLD AR NONE RETIRED 300.0 20-Jun-11 1data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1001732 entries, 0 to 1730 Data columns (total 7 columns): cand_nm 1001732 non-null object contbr_nm 1001732 non-null object contbr_st 1001728 non-null object contbr_employer 988003 non-null object contbr_occupation 993302 non-null object contb_receipt_amt 1001732 non-null float64 contb_receipt_dt 1001732 non-null object dtypes: float64(1), object(6) memory usage: 61.1+ MB 各字段含义 cand_nm — 接受捐赠的候选人姓名 contbr_nm — 捐赠人姓名 contbr_st — 捐赠人所在州 contbr_employer — 捐赠人所在公司 contbr_occupation — 捐赠人职业 contb_receipt_amt — 捐赠数额（美元） contb_receipt_dt — 收到捐款的日期 1data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } contb_receipt_amt count 1.001732e+06 mean 2.982358e+02 std 3.749665e+03 min -3.080000e+04 25% 3.500000e+01 50% 1.000000e+02 75% 2.500000e+02 max 2.014491e+06 2.数据清洗2.1缺失值处理dataframe.fillna()函数，对缺失值进行处理共有5个参数，功能是用指定的方法填充NA/NaN value: 变量，字典，Series or Dataframe method: {‘backfill’,’bfill’,’pad’,’ffill’,None}，默认值为None，pad/ffill表示用前一个非缺失值去填充缺失值backfill/bfill表示用下一个非缺失值填充该缺失值，None指定一个值去代替缺失值 inplace: True or False，True表示在原始数据上修改，False表示创建一个副本在副本上修改 limit: int，表示连续填充最大值，默认为None 123# 将两个字段用NOT PROVIDED填充data['contbr_employer'].fillna('NOT PROVIDED', inplace=True)data['contbr_occupation'].fillna('NOT PROVIDED', inplace=True) 1print('一共有&#123;&#125;位候选人'.format(len(data['cand_nm'].unique()))) 一共有13位候选人 12# 类似SQL中的distinct操作data['cand_nm'].unique() array([&#39;Bachmann, Michelle&#39;, &#39;Romney, Mitt&#39;, &#39;Obama, Barack&#39;, &quot;Roemer, Charles E. &#39;Buddy&#39; III&quot;, &#39;Pawlenty, Timothy&#39;, &#39;Johnson, Gary Earl&#39;, &#39;Paul, Ron&#39;, &#39;Santorum, Rick&#39;, &#39;Cain, Herman&#39;, &#39;Gingrich, Newt&#39;, &#39;McCotter, Thaddeus G&#39;, &#39;Huntsman, Jon&#39;, &#39;Perry, Rick&#39;], dtype=object) 1234567891011121314parties = &#123;'Bachmann, Michelle': 'Republican', 'Cain, Herman': 'Republican', 'Gingrich, Newt': 'Republican', 'Huntsman, Jon': 'Republican', 'Johnson, Gary Earl': 'Republican', 'McCotter, Thaddeus G': 'Republican', 'Obama, Barack': 'Democrat', 'Paul, Ron': 'Republican', 'Pawlenty, Timothy': 'Republican', 'Perry, Rick': 'Republican', "Roemer, Charles E. 'Buddy' III": 'Republican', 'Romney, Mitt': 'Republican', 'Santorum, Rick': 'Republican'&#125;data['party'] = data['cand_nm'].map(parties) 1data['party'].value_counts() Democrat 593747 Republican 407985 Name: party, dtype: int64 排序，按照职业汇总对赞助金额进行排序按照各个职位进行汇总，计算赞助总金额展示前20项，发现不少职业是相同的，只不过表达方式不一样，如C.E.O和CEO DataFrame.sort_value(by,ascengding=True,inplace=False) by表示根据那一项排序，可以传入多列值 ascending=True表示升序，False表示降序，inplace=True表示修复原dataframe，默认为False 1data.groupby('contbr_occupation')['contb_receipt_amt'].sum().sort_values(ascending=False)[:20] contbr_occupation RETIRED 48176397.00 ATTORNEY 18470473.30 HOMEMAKER 17484807.65 INFORMATION REQUESTED PER BEST EFFORTS 15859514.55 INFORMATION REQUESTED 8742357.59 PHYSICIAN 7224044.40 PRESIDENT 6347843.59 EXECUTIVE 5273717.90 CONSULTANT 4932627.98 NOT PROVIDED 4224760.39 CEO 3570942.20 LAWYER 3537982.19 OWNER 3278488.16 INVESTOR 3204481.92 ENGINEER 2730527.43 PROFESSOR 2458033.81 C.E.O. 2433218.11 SELF-EMPLOYED 2259150.94 MANAGER 2167571.47 REAL ESTATE 2110499.34 Name: contb_receipt_amt, dtype: float64 12]]></content>
  </entry>
  <entry>
    <title><![CDATA[hive的基本操作]]></title>
    <url>%2F2019%2F09%2F17%2Fhive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[表的增删改查操作创建表使用if not exists 如果存在则跳过，comment为注释。123456789create table if not exists mydb.employees( name string comment 'Employee name', salary float comment 'Employee salary', subordinates array&lt;string&gt; comment 'Names of subordinates', deduction map&lt;string, float&gt;, address struct&lt;street:string, city:string, state:string, zip:int&gt; comment 'Home address')comment 'descriptions of table'location '/user/hive/warehouse/mydb.db/employees'; 描述表显示表的字段和结构，使用desc/describe 1234--显示表的字段和数据类型desc table_name;--显示对应字段的数据类型desc table_name.columns 管理表和外部表管理表是hive创建的表，由hive控制其生命周期，hive默认情况下会将数据存在在配置文件指定的目录当中，由hive.metastore.warehouse.dir指定。当使用hive删除表的时候，对应的数据也会被删除，即hdfs文件系统中的数据也会被删除。管理表的缺点在于无法共享数据，比如利用pig等工具创建的数据，hive对其没有权限。当使用hive查询这些数据的时候就可以使用一个外部表指向这份数据，而不需要对其的权限。外部表需要使用external修饰。 123456789101112create external table if not exists stocks( exchange string, symbol string, ymd string, price__open string, price__high string, price__low string, price__close float, volume int, price_adj_close float)row format delimited fields terminated by ',' '分隔符为,'location 'data/stocks'; 加上external字段值后，删除表并不会删除这份数据，不过描述标的元数据信息会被删除。元数据可以理解为对该表的描述信息，而不是表内数据。 需要注意的是如果语句省略了external关键字同事源表是外部表，那么新表也是外部表，如果源表是管理表，新表也是管理表。在加上external之后，无论源表是管理表还是外部表，新表都是外部表。 分区表在建表过程中，会根据分区字段创建对应目录，优点在于分层存储，可以加快查询速度，而缺点在于一些数据存在于文件目录下，但是hive只能从表中读取数据，因此会造成资源浪费。分区表创建： 12345678create table employees( name string, salary float, subordinates array&lt;string&gt;, deduction map&lt;string,float&gt;, address struct&lt;street:string, city:string, city:string, state:string&gt;)partitioned by (country string, state string); 在建表的时候hive在hdfs上的目录为…/employees/country/state 查看表中存在所有分区 1show partitions employees; 查询指定分区 1show partitions employees partition(country='CHINA') 删除表1drop table if exists table_name; 对于管理表，表的元数据和表内数据都会被删除。 修改表 表的重命名将表从ａ重命名为ｂ 1alter table a rename to b; 增加表分区1234567alter table add partiion--在一个查询语句中增加多个分区alter table table_name if not exists partition(...) location '/user/hive/warehouse/a'partition(...) location '/user/hive/warehouse/b'partition(...) location '/user/hive/warehouse/c' 修改列的信息将列名从a改到b，并且将其移到serverity字段后面。 1234alter table table_name change column a b type_name '修改列的数据类型'comment 'xxx'after serverity 增加新的列1234alter table table_name add column( app_name string comment 'Application name', session_id long comment 'the current session id';) 删除或替换列1234alter table table_name replace columns（ hour_mins_secs INT comment 'xxx' severity string comment 'xxx';) 将之前的列都删除，只留下replace的列 修改表的属性1alter table table_name set tblproperties('notes'='xxx'); 修改表的存储属性1alter table table_name partition(a=xxx,b=xxx,c=xxx) set fileformat sequencefile; 指定对应的分区中的表，并且重新设置其格式。 加载和导出数据从本地加载数据123load data local inpath '/home/hadopp/data.txt'overwrite into table employeespartition(country='US',state='CA'); 需要注意的是创建分区表的时候使用的是partition by。如果目录不存在，hive会先创建分区目录。 通过查询语句加载数据123insert overwrite table employeespartition(country='US',state='CA')select * from table_name where xxx=xxx; 通过查询语句建表12create table if not exists table_name as select * from table_name_b; 导出数据12345--方法一，谁用hadoop提供的工具hadoop fs -cp source_path target_path--方法二insert overwrite local directory '/home/hadoop/employees'select * from employees; hive的连接操作table stu 12341 chenli 212 xuzeng 223 xiaodan 234 hua 24 table sub 12341 chinese2 english3 science5 nature 内连接inner join，关键字 join on。仅列出两个表中符合连接条件的数据。 12345select a.*,b.* from stu a join sub b on a.id=b.id--结果1 chenli 21 1 chinese2 xuzeng 22 2 english3 xiaodan 23 3 science join后面连接表，而on指定连接条件。 左连接和右连接左连接，显示左边表的所有数据，如果右边表有与之对应的数据则显示，否则显示为NULL。 123456select a.* from stu a left outer join sub b on a.id=b.id;--结果1 chenli 21 1 chinese2 xuzeng 22 2 english3 xiaodan 23 3 science4 hua 24 NULL NULL 右连接与左连接相反，使用的关键字为 right outer join xxx on xxx。 标准查询关键字执行顺序为 from-&gt;where-&gt;group by-&gt;having-&gt;order by。 全连接左表和右表都显示，如果没有对应数据，则都显示为NULL 1234567select a.* from stu a full outer join sub b on a.id=b.id;--结果1 chenli 21 1 chinese2 xuzeng 22 2 english3 xiaodan 23 3 science4 hua 24 NULL NULLNULL NULL NULL 5 nature 左半开连接左半开连接。left semi join，语法与左连接不一样，只能选择出左边表的数据，此数据符合on后面的条件。 1234567select a.* from stu a left semi join sub b on a.id=b.id;--结果1 chenli 212 xuzeng 223 xiaodan 23--下列语句会报错select a.*,b.* from stu a left semi join sub b on a.id=b.id; 笛卡尔连接123456789101112131415161718select a.*,b.* from cl_student a join cl_stu_sub b;--结果1 chenli 21 1 chinese1 chenli 21 2 english1 chenli 21 3 science1 chenli 21 5 nature2 xuzeng 22 1 chinese2 xuzeng 22 2 english2 xuzeng 22 3 science2 xuzeng 22 5 nature3 xiaodan 23 1 chinese3 xiaodan 23 2 english3 xiaodan 23 3 science3 xiaodan 23 5 nature4 hua 24 1 chinese4 hua 24 2 english4 hua 24 3 science4 hua 24 5 nature 花了几天的时间整理了hive的用法，终于不用在对着SQL摸瞎了，加油吧进击的SQL boy！ 日常福利(●´∀｀●)]]></content>
      <categories>
        <category>大数据</category>
        <category>hive编程</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive数据类型]]></title>
    <url>%2F2019%2F09%2F08%2Fhive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[文本文件数据编码 TSV：tab separated values；即“制表符分隔值”，用制表符分隔数据 CSV： comma separated values；即“逗号分隔值”，用逗号分隔数据 两种文件存在的缺点在于文件中可能存在不需要作为分隔符的逗号或者制表符存在，所有hive有专门的分隔符。hive记录中默认的分隔符 分隔符 描述 \n 对于文本文件来说每一行都是记录，可以使用换行符作为分隔符 ^A(ctrl+A) 用于分隔字段(列)，在CREATE TABLE 语句中可以使用八进制编码\001表示，键盘上打不出来。 ^B 用于分隔array或者struct中的元素，或于用map钟键值对的分隔，在CREATE TABLE中使用\002表示 ^C 用于MAP钟键与值的分隔，用\003表示 读时模式传统数据库是写时模式，即在写入文件的时候会，会对数据的格式进行校验，如果不符合，将无法写入数据库。 hive是读时模式，在往数据库里写入不会对数据进行校验，但是在读取数据的时候会对数据进行校验，对于不合格的数据，会设置为null。 hive的优点在于加载(写)数据的时候速度较快，因为不需要对数据进行解析，而传统写时模式则有利于数据的查询。 好久没有更新博客了，这篇虽然水了点，写得像个笔记，算是开篇吧，福利就上亚丝娜吧!!!]]></content>
      <categories>
        <category>大数据</category>
        <category>hive编程</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>编码类型</tag>
      </tags>
  </entry>
</search>
