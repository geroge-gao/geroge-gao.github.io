<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《深度学习推荐系统》阅读笔记(2)]]></title>
    <url>%2F2020%2F08%2F26%2F%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[Embedding作用及应用Embedding也是编码方式的一种，主要作用是将稀疏向量转换成稠密向量。Embedding是深度学习的基础核心操作，主要有一下三个原因1、推荐场景下会使用one-hot对类别和id进行编码，造成大量稀疏数据，神经网络无法很好地处理这种稀疏数据2、Embedding本身是极强的特征向量，可以引入任何信息进行编码。 3、Embedding对物品、用户的相似度计算是常用的推荐系统召回技术。 所以Embedding对应的应用方向主要有一下三个： 作为Embedding层，完成高维稀疏特征向量到低维稠密特征向量的转换 作为预训练的特征向量，在与其他特征向量连接之后，一同输入深度学习网络进行训练 计算物品和用户的相似度做Embedding Word2Vec和Item2VecWord2vec主要用于计算词向量相似度，Item2Vec是Word2Vec在推荐上面的应用，只是两者优化目标不同。并且Item2Vec使用的物品序列是由用户历史行为产生的物品序列。 假设Word2Vec中有一个长为$T$的句子为$w_1,w_2,…,w_T$,其优化的目标为 \frac{1}{T}\sum_{t=1}^T\sum_{-c\le j\le c,j\ne0}\log p(w_{t+j}|w_t)而对于Item2Vec，w表示历史记录，其优化目标为: \frac{1}{K}\sum_{i=1}^K\sum_{j\ne i}^K\log p(w_j|w_i)Item2Vec摒弃了时间窗口，认为任意两个物品都是相关的，而不仅仅是时间窗口之类的物品对数和。 缺点：只能利用序列型数据，对网络化数据捉襟见肘 Graph EmbeddingGraph Embedding是一种对图结构中的结点进行Embedding编码的方法。 DeepWalk主要思想是在由物品组成的图上随机游走，产生大量物品序列，然后将这些序列利用Word2Vec进行训练，得到物品的Embedding向量。 DeepWalk算法流程 1、基于用户行为构建关系图，例如用户先买A再买B，就会产生A到B的有向边，一次类推，将用户行为构建成物品关系图。 2、采用随机游走算法选择起始点，重新产生物品序列。 3、将产生序列输入到Word2Vec模型中 在DeepWalk算法流程中，唯一需要形式化定义的就是随机游走的跳转概率。假设遍历到节点$v_i$，那么跳转到其邻节点的概率是$v_j$的概率为： P(v_j|v_i)=\begin{cases} \frac{M_{ij}}{\sum_{j\in N_+(v_i) M_{ij}}}, & v_j\in N_+(v_i) \\ 0, & e_{ij}\notin \varepsilon \end{cases}其中$M_{ij}$是节点$v_i$到$v_j$的边的权重。 优点：简单 缺点：随机游走抽样性不强 Node2vec通过调整随机游走权重的方法使Graph Embedding的结果更加趋向于体现网络的同质性或结构。网络的同质性指距离相近的节点的Embedding应尽量相似。结构性是指结构上相似的节点Embedding应尽量相似。 为了表达网络的结构性，游走更加倾向于BFS。对于BFS，不同的节点例如局部中心节点、边缘节点、连接性节点，其生成的序列节点数量和顺序必然不同。 为了表达同质性，游走通常采用DFS。DFS通常在一个大的集团内部游走，这就使得社区内部节点Embedding更加相似。 Node2vec主要是通过节点间的跳转概率空值游走时DFS和BFS的倾向 如图所示(图先欠着)，Node2Vec算法从节点$t$跳到节点$v$，再从$v$跳节点$x$的概率为$\pi_{vx}=a_{pq}(t,x)\cdot w_{vx}$ a_{pq}=\begin{cases} \frac{1}{p},& 如果d_{tx}=0 \\ 1, &如果d_{tx}=1 \\ \frac{1}{q}, &如果 d_{tx}=2 \end{cases}其中$d_{tx}$表示节点$t$到节点$x$的距离，参数$p$和$q$共同控制这随机游走的倾向性，当$p$月小的时候，随机游走返回$t$的可能性越高，当$q$越小的时候，随机游走返回远方的可能性越大。 直观理解：同质性的物品很有可能是相似的物品或者经常被一同购买的物品，结构性可能是各类爆款、各品类的最佳凑单商品等拥有类趋势或结构性的商品。 优点：可以有效的挖掘不同的网络特征 缺点：需要较多的人工调参工作 EGESEnhanced Graph Embedding with Side information基本思想是在DeepWalk生成Graph Embedding的基础上引入补充信息，以解决冷启动问题。 可以利用物品相同属性、相同类别等信息建立物品之间的边，生成基于内容的知识图谱，基于知识图谱生成的物品Embedding向量作为补充信息，并且采用平均池化将不同物品的Embedding平均起来，为了防止平均池化导致有效信息丢失，对每个Embedding向量进行加权(Enhance 的体现)。在实际加权的过程中，采用了$e^{a_j}$而不是$a_j$进行加权，主要原因有两点：一：避免权重为0，二：指数函数在梯度下降的过程中有良好的数学性。因此EGES更多的是工程上的尝试，缺乏学术上的创新。 局部敏感哈希传统Embedding在做召回的时候，以为着要对所有物品进行Embedding进行遍历，效率比较低。在推荐系统中，通常物品总数动达几百万，因此会导致线上模型过程的巨大延迟。 局部敏感哈希则是为了解决这个问题，其基本思想是然相邻的点落入到同一个桶里面，]]></content>
      <categories>
        <category>机器学习</category>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>Embedding</tag>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深度学习推荐系统》阅读笔记(一)]]></title>
    <url>%2F2020%2F08%2F24%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[传统模型协同过滤1、User-CF的缺点 一般用户数量远大于物品数，用户相似度矩阵较大，不便于存储。同时用户数增长会造成在线存储系统难以承载扩张速度 用户历史数据向量往往比较稀疏，User-CF不适用于正反馈获取比较困难的场景，例如大件商品购买等低频应用 2、User-CF和Item-CF的使用场景 User-CF适用于新闻推荐场景，具有较强的社交性、更容易发现热点 Item-CF适用于兴趣变化比较稳定的应用，比如电商和视频推荐场景 协同过滤存在天然缺陷，头部效应明显，处理稀疏向量的能力较弱。并且协同过滤仅仅利用了用户年龄、性别、商品描述、商品分类等信息，会造成有效信息遗漏。 矩阵分解矩阵分解，针对协同过滤算法头部效应明显，泛化能力弱，加入了隐向量 矩阵分解优点： 泛化能力强 空间复杂度低，只需要存储用户和物品隐向量 更好的扩张性和灵活性，可以和其他特征拼接 缺点： 和协同过滤一样，无法利用用户年龄性别等信息 逻辑回归将推荐问题转换成CTR问题 优点： 数学上的支撑 可解释性强 符合人类对预估过程的直觉认识 缺点： 表达能力不强，无法进行特征交叉，导致信息丢失 FM与FFM辛普森悖论：在分组中占优势的一方在总评中不占优势。 POLY2暴力交叉特征，对所有特征进行两两组合，数学形式： \Phi POLY2(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^nw_h(j_1,j_2)x_{j_1}x_{j_2}$x_i$和$x_j$表示特征$i,j$，$w_h(i,j)$表示特征的权重赋值，本质任是线性模型。 有点： 本质任然是线性模型，便于工程兼容 缺点： 采用one-shot编码，处理类别特征是会导致特征极度稀疏，并且无法选择特征交叉，导致部分稀疏特征更加稀疏，无法收敛 参数上升了，量级从$n$上升至$n^2$。 FM数学形式： FM(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n(w_{j_1}\cdot w_{j_2})x_{j_1}x_{j_2}$w$为$(1,k)$维向量。 优点： 引入隐向量使FM有效解决数据稀疏性问题，将权重模型减少到了$n*k$ 可以使用梯度下降学习，不失灵活性和 FFM相对于FM，引入了特征感知域的概念，使得模型的表达能力更强 FFM(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n(w_{j_1,f_2}\cdot w_{j_2,f_1})x_{j_1}x_{j_2}FFM和FM的区别在于隐向量由原来的$w_{j_1}$变成了$w_{j_1,f_2}$，意味着每个特征对应着不是唯一的隐向量，而是一个隐向量组。吧 GBDT+LR自动化特征的开端 1、GBDT进行特征转换 将样本输入一个子树之后，回落到一个叶子结点中，将该节点置为1，其他结点置为0，然后将所有结点进行拼接，就得到一个特征向量 决策树的深度决定交叉特征的的阶数，决策树深度为4，经过3次分裂，交叉特征为3阶 传统特征工程： 进行人工或者半人工的特征组合和筛选 改造目标函数，改进模型结构，提升特征交叉的方式增强特征组合 弊端： 对算法工程师经验和经历投入要求太高了 从根本上改变模型结构，对模型设计能力要求太高 GBDT+LR的优点：特征工程由独立模型完成，实现端到端的训练。 深度学习模型深度学习模型优点： 1、表达能力更强，能够挖掘出更多的数据中潜藏的模式。 2、模型结构灵活，能够根据业务场景和数据特点灵活调整模型结构。 深度学习模型的几个发展方向 改变神经网络复杂度 改变特征交叉的方式 组合模型 FM模型的学习演化版本 注意力机制和推荐模型的结合 序列模型与推荐系统的结合 强化模型与推荐系统的结合 AutoRec 浅层神经网络，对于协同过滤，存在$m\times n$的共现矩阵，相当于神经网络中的权重$w$，使用单层神经网络具有一定的泛化能力，但是结构较简单，泛化能力存在不足。 Deep Crossing微软搜索引擎中的搜索广告推荐场景。 Deep Crossing需要解决的三个问题 1、离散类特征过于稀疏，不利于直接输入神经网络进行训练，如何解决稀疏特征稠密化？ 引入embedding层，将稀疏特征稠密化 2、如何解决特征自动交叉问题？ 引入残差网络结构，通过多层残差对各个维度进行交叉&gt;。 3、如何在输出层达成问题设定的优化目标？ 引入Scoring层，对于CTR二分类问题，采用逻辑回归模型，对于图像类多分类问题，采用softmax模型。 优点： 没有采用任何人工特征的参与。 NeuralCFNeuralCF采用多元神经网络+输出层结构代替了矩阵分解模型中简单內积操作，目的是： 让用户向量和物品向量充分交叉，得到价值更多的特征组合信息 引入更多非线性特征，让模型表达能力更强 优势： 利用神经网络的优势，可灵活组合不同的特征，按需增加或减少模型复杂度 缺点： 由于基于协同过滤的思想，没有引入其他类别特征。 PNN相对于Deep Cross，将stacking层换成了Product Layer，乘积层，里面既有内集，也有外集。 优点： 对于Embedding向量多样化交叉，定义了內积和外积操作 缺点： 特征进行了无差别交叉，一定程度的忽略了原始特征向量中包含的价值信息。 Wide&amp;DeepWide&amp;Deep设计的初衷和最大价值在于同时具有较强的记忆能力和泛化能力 记忆能力：模型直接学习并且利用历史数据中的物品和特征共现能力，例如协同过滤，可以通过历史数据计算物品和用户共现矩阵，进而通过历史数据进行推荐。 泛化能力：模型的传递特征的相关性，发觉稀疏甚至从未出现过的稀有特征和最终标签相关性能力。 Deep部分输入的是全量的特征，数值型和类别特征的embedding向量通过全连接层连接在一起。 Wide部分仅输入已安装应用和曝光应用两类特征，已安装应用表示用户历史行为，曝光应用代表当前待推荐的应用，选择这两类特征的原因是充分发挥wide的记忆能力强的有点。 Wide&amp;Deep模型的优点： （1）抓住业务本质特点，融合传统模型的记忆能力和深度模型的泛化能力 （2）模型结构不复杂，容易工程化实现 缺点： Wide部分任需要人工筛选特征 Wide&amp;Cross使用Cross网络替代原来的wide部分。设计Cross网络的目的是为了增加特征之间的交互力度，使用多层交叉层对输入向量进行特征交叉。减少人工特征。 假设$l$层交叉层的输出向量为$x_l$，那么$l+1$层的向量输出为: x_{l+1}=x_0x_l^TW_l+b_l+x_l每层均保留了输入向量，因此在输入向量和输出向量之间的差别不是特别大。 优点：可以自动进行交叉特征，避免了很多基于业务理解的人工特征。 缺点：Cross部分网络复杂度较高。 FM系列FNN为了解决Embedding训练速度慢的问题，FNN采用的思路是用FM模型训练好的个特征隐向量初始化Embedding层参数，有点类似预训练。FM的计算公式： y_{FM}=sigmod(w_0+\sum_{i=1}^Nw_ix_i+\sum_{i=1}^N\sum_{j=i+1}^Nx_ix_j优点：利用FM初始化Embedding层参数，加快训练 缺点：结构比较简单，没有针对性的特征交叉层 DeepFM采用FM优化Wide&amp;Deep模型中Wide模块，主要针对Wide部分不具备自动组合特征的能力的缺陷进行改善的。利用FM替换了原来的Wide部分，加强了浅层网络部分特征组合的能力。左边FM与右边神经网络共享相同的embedding层。FM对不同特征域两两交叉。也就是将embedding层当做原FM中隐向量。 NFMNFM的思想，利用神经网络改进FM，主要思路是利用一个表达能力更强的函数替代原FM \hat y_{FM}(x)=w_0+\sum_{i=1}^Nw_ix_i+\sum_{i=1}^N\sum_{j=i+1}^Nv_i^Tv_j\cdot x_ix_j \hat y_{MFM}(x)=w_0+\sum_{i=1}^Nw_ix_i+f(x)主要有5各模块 原始稀疏特征向量 Embedding层 特征交叉池化层 隐层 预测层 其实就是在Embedding层和多层神经网络之间增加了特征交叉池化层，具体操作如下： f_{BI}(V_x)=\sum_{i=1}^n\sum_{j=i+1}^n(x_iv_i)\odot (x_jv_j)$\odot$表示两个向量的元素积操作 (v_i\odot v_j)_k=v_{ik}v_{j_k}基于FM的深度学习模型的有点和局限性 优点： 让模型具备非常强的非线性表达能力 局限： 进行了大量基于不同特征交互操作思路的尝试，特征工程的思路已经穷尽了。 AFM在NFM模型中，对于不同的特征一视同仁，但是实际上不同的特征对于业务的影响是不一致的，因此AFM通过在NFM的特征交叉层和最终输出层加上注意力机制实现，AFM的特征交叉过程同样采用元素积操作 f_{PI}(\delta) =\{(v\odot v_j)x_ix_j\}_{(i,j)\in R_{x}}AMF加入注意力得分之后的池化过程 f_{Att}(f_{PI}(\delta))=\sum_{(i,j)\R_x}a_{i,j}(v_i\odot v_j)x_ix_j为了防止交叉特征数据稀疏问题带来的权重难以收敛，AFM在交叉层和池化层之间的注意力网络来生成注意力得分 DIN应用场景： 阿里巴巴电商广告推荐 DIN模型是在深度学习网络中加入了注意力机制，利用候选商品和历史行为相关性计算出一个权重，这个权重代表注意力的强弱。例如，广告中的商品是键盘，用户点击商品序列有几个不同的商品id，分别为鼠标、T恤和洗面奶。因此鼠标这个历史行为的商品id对于预测键盘广告的点击率重要程度大于后两者。 注意力形式化表达： V_u=f(V_a)=\sum_{i=1}^Nw_i\cdot V_i = \sum_{i=1}^Ng(V_i,V_a)\cdot V_i$V_u$是用户Embedding向量，$V_a$是候选广告商品Embedding向量，$V_i$是用户$u$第$i$次行为Embedding向量。用户行为指的是游览的商品和店铺。$w_i$由$V_i$和$V_a$的关系决定，即$w_i=g(V_i,V_a)$, g函数采用的是注意力激活单元。 优点： 在传统深度学习推荐系统模型基础上，引入了注意力机制，并利用用户历史行为和目标广告商品的相关性计算注意力得分 缺点： 没有充分利用历史行为以外的其他特征。 DIEN相比DIN，考虑到了时序信息，用户的兴趣也是随着时间变化的，因此时序信息能够 1、加强最近行为对下一次购买的预测影响 2、序列模型能够学习到购买趋势 网络结构还是输入层+Embedding层+连接层+多层全连接层+输出层 DIEN的创新点在于构建了一个兴趣进化网络。 兴趣进化网络分三层： 1、行为序列层：将原始的ID行为序列转换成Embedding行为序列 2、兴趣抽取层：通过模拟用户兴趣迁移过程，抽取用户兴趣。采用GRU，相对于LSTM参数更少 3、兴趣进化层：通过在兴趣层基础上加上注意力机制，模拟兴趣的进化过程。结构为AUGRU，在院GRU的更新门的结构上加上了注意力得分。 优点： 考虑到了时序信息 缺点： 序列模型训练起来比较复杂]]></content>
      <categories>
        <category>机器学习</category>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>阅读笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程之特征选择]]></title>
    <url>%2F2020%2F08%2F05%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[​ 特征选择，即对于生成的特征集合进行筛选，得到一个子集，主要有一下三个目的 简化模型，增加可解释性 改善性能，并且节省存储空间和计算开销 改善通用性，降低过拟合风险。 特征选择主要分为三种方法，过滤方法、封账方法和嵌入方法过滤方法Filter过滤方法主要特点： 不依赖与机器学习算法 一般分为单变量和多变量 单变量一般基于特征变量与目标变量之间的相关性或互信息，根据相关性排序，过滤掉最不相关特征 多变量有基于相关性和一致性的特征选择 单变量覆盖率一般来说，会把缺失率15%-20%左右的特征丢弃掉，不过不是绝对的，也可以通过其他特征的关系或者利用模型预测对缺失值进行补齐。 1234cols = data.columns.values.tolist()missing_rate = pd.DataFrame(&#123;'column_name':cols&#125;)for c in cols: missing_rate.loc[missing_rate['column_name']==c,'missing_rate'] = data[data[c].isna()==True].shape[0]/data.shape[0] 皮尔逊相关系数皮尔逊相关性系数用于度量两个变量$X$和$Y$的线性相关性，计算公式为 \rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}样本上的相关性系数为 r=\frac{\sum_{i=1}^n(X_i-\bar X_i)(Y_i-\bar Y_i)}{\sqrt{(\sum_{i=1}^n(X_i-\bar X_i)^2)}\sqrt{(\sum_{i=1}^n(Y_i-\bar Y_i)^2)}}在数据标准化（ ）后，Pearson相关性系数、Cosine相似度、欧式距离的平方可认为是等价的。 如何理解皮尔逊相关系数（Pearson Correlation Coefficient）？ - 微调的回答 - 知乎 https://www.zhihu.com/question/19734616/answer/349132554 使用方法 123from scipy.stats import pearsonrr,p = pearsonr(X,Y) r表示相关性系数 r&gt;0表示正相关 r=1表示正线性相关 r&lt;0表示负相关 r=0表示非线性相关。 r=-1表示负线性相关 p-value越低表示越低，可靠性越高。https://www.cnblogs.com/lijingblog/p/11043513.html Fisher得分在分类问题中，对于好的特征，在同一类别中取值比较相似，在不同类别中取值差异较大。因此特征i的重要性可以用Fisher得分$S_i$表示： S_i=\frac{\sum_{j=1}^Kn_j(u_{ij}-u_i)^2}{\sum_{j=1}^Kn_j\rho^2_{ij}}$u_{ij}$和$\rho_{ij}$分别表示特征$i$在类别$j$中的中的均值和方差，$n_j$表示类别$j$的样本数，Fisher特征越高，特征在不同类别的差异性越大，在同一类别的差异性越小。 卡方检验目的：检验特征变量和目的变量之间的相关性 公式为： \chi^2=\sum_{i=1}^r\sum_{j=1}^c\frac{(O_{i,j}-E_{i,j})^2}{E_{i,j}^2}其中$O_{i,j}$表示观测值，$E_{i,j}$表示期望值，$i$表示类别$i$，$j$表示对应的目标变量$j$。如何假设正确，$\chi$越小，相关性越小。 1234567from sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2from sklearn.datasets import load_irisiris = load_iris()model1 = SelectKBest(chi2, k=2)#选择k个最佳特征model1.fit_transform(iris.data, iris.target) 互信息用于度量两个变量之间的相关性，互信息越大，表示两个变量的相关性越高，互信息为0，表示两个变量相互独立。 互信息计算公式： I(X,Y)=\sum_{x\in X}\sum_{y\in Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}其中$X、Y$的联合分布为$p(x,y)$，边缘分布为$p(x),p(y)$。 12from sklearn import metrics as mrmr.mutual_info_score(Y,X) 多变量最小冗余最大相关性mRmR相关特征选择(CFS)相关特征选择(Correlation Feature Selection，CFS)基于以下假设来评估集合的重要性：好的特征集合包含于目标变量非常相关的特征，但这些特征彼此不想关。 公式如下： CFS=\max_{S_k}[\frac{r_{cf_1}+r_{cf_2}+...+r_{cf_k}}{\sqrt{k+2(r_{f_1f_2}+r_{f_if_j}+...+r_{f_kf_1})}}其中$r_{cf_i}$和$r_{f_if_j}$分别表示特征变量和目标变量之间的相关性以及特征变量与特征变量之间的相关性。 这部分代码暂时没有搞懂，后续在研究。 1234567891011121314import numpy as npfrom skfeature.function.statistical_based import CFSfrom sklearn.datasets import load_iris # 利用iris数据作为演示数据集# 载入数据集iris = load_iris()X, y = iris.data, iris.targettrain_set = X[0:100,:]test_set = X[100:,]train_y = y[0:100]num_feature = 2 # 从原数据集中选择两个变量feature_index = CFS.cfs(train_set, train_y) FCBFFCBF算法： 全称 Fast Correlation-Basd Filter Solution, 是一种快速过滤的特征选择算法，一种基于symmetrical uncertainty（SU）的方法。其计算步骤如下： 1、计算每个特征与目标C之间的相关性$SU_{F_i,c}$ SU(X,Y)=2\frac{IG(X,Y)}{E(X)+E(Y)} \\E(x)=-\sum_{i=1}^cPx_i()*\log_2(P_{X_i}) \\E(X|Y)=-\sum_{i=1}^{C_Y}P(y_i)\sum_{j=1}^cP(x_j|y_i)\log_2(P(x_j|y_i))其中$IG(X,Y)$代表信息增益，$E(X)$代表信息熵，$P(X_i)$表示$X$取到$i$的概率，$c$为类别数目 2、将最大相关性的特征预先使用$\delta$选择出来 3、将$SU_{F_i,c}$按照大小排序，并且计算每个特征$F_i$与排序中SU小于$SU_{}$的特征$F_j$之间的相关性$SU_{F_i,F_j}$，如果$SU_{F_i,c}&gt;SU_{F_j,c}$,计算$SU_{F_i,F_j}$ 4、如果$SU_{F_i,F_j}&gt;SU_{F_j,c}$，删除特征$F_j$ 封装方法Wrapper封装方法直接利用机器学习算法评价特征子集的效果，它可以检测两个或者多个特征之间的相互关系，而且选择的特征子集让模型效果运行达到最优 确定性算法序列前向特征选择(SFS)和序列后向特征选择(SBE)序列向前算法，特征子集从空集开始，每次只加入一个特征， 随机算法退火算法遗传算法遗传算法步骤： 1、初始化种群 一个种群有好几条染色体，假设有m个初始特征，那么染色体为一个m*1的一维向量[0,1,0,1,….1]，全部由0或者1组成，初始化时，0和1随机选择。 2、评估种群中个体适合度 用交叉检验cross_val_score(个体，y)的结果作为适应度。适应度计算类似LDA 3、选择 每条染色体的适应度不同，被选择的概率也不同，用轮盘赌选择，先生成与染色体个数相同的随机数个数，先生成与染色体个数（种群大小）相同的随机数然后再一个个看这些随机数落在哪个染色体的范围内例：染色体的选择概率：①[0,0.3), ②[0.3,0.6), ③[0.6,0.7), ④[0.7,0.9), ⑤[0.9,1]生成的随机数：0.2, 0.4, 0.5, 0.78, 0.8被选中的染色体：①, ②, ②, ④, ④ 5、交叉 若第i条与第i+1条染色体发生交叉，随机选择交叉点，然后交叉。 例如 父染色体 a:[0,1,0,0,1] b:[1,0,1,1,1] 交叉之后的染色体为[0,1,0,1,1] 6、变异 染色体的某个点取反。目的是防止局部最优 蚁群算法蚁群算法的基本思想： 1、蚂蚁在路径上释放信息素。 2、碰到还没走过的路口，就随机挑选一条路走。同时，释放与路径长度有关的信息素。 3、信息素浓度与路径长度成反比。后来的蚂蚁再次碰到该路口时，就选择信息素浓度较高路径。 4、最优路径上的信息素浓度越来越大。 5、最终蚁群找到最优寻食路径。 嵌入方法Embedded嵌入方法将特征选择嵌入到模型的构建过程当中，具有封装方法与机器学习方法相结合的有点，而且具有过滤方法计算效率高的有点，避免了前面两种方法的不足。 LASSO方法及线性回归+L1范数，具体原理就不细讲了。 123456789101112from sklearn.linear_model import Lassofrom sklearn.preprocessing import StandardScalerfrom sklearn.datasets import load_bostonboston = load_boston()scaler = StandardScaler()X = scaler.fit_transform(boston["data"])Y = boston["target"]names = boston["feature_names"]lasso = Lasso(alpha=.3)lasso.fit(X, Y) 基于树模型的特征选择树模型本身可以进行特征选择，配合sklearn中的SelectFromModel可以进行特征选择。 1、结合SelectFromModel SelectFromModel的作用是训练基础模型，得到系数较高的特征 1234567from sklearn.feature_selection import SelectFromModelfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.datasets import load_iris iris = load_iris()gbdt = GradientBoostingClassifier()SelectFromModel(gbdt).fit_transform(iris.data,iris.target) 2、结合相关系数 主要分为一下几步： 第一次训练lightGBM，得到特征重要性 挑选重要性比较高的特征，计算特征相似度，热力图也行 筛选掉相似度过高的特征 将筛选之后的特征送入lightGBM重新训练 参考资料 1、https://www.cnblogs.com/hhh5460/p/5186226.html 2、https://www.kaggle.com/juliaflower/feature-selection-lgbm-with-python/comments 3、美团机器学习实践 4、https://blog.csdn.net/u012017783/article/details/71872950 5、https://www.cnblogs.com/holaworld/p/12631851.html 6、https://zhuanlan.zhihu.com/p/33042667 7、https://www.cnblogs.com/heaad/archive/2010/12/20/1911614.html 8、https://www.cnblogs.com/holaworld/p/12631933.html]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>数据挖掘</tag>
        <tag>特征选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征工程之特征构建]]></title>
    <url>%2F2020%2F07%2F11%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%89%B9%E5%BE%81%E6%9E%84%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[类别特征 几乎总是需要做一些处理 类别特征种类过多会造成稀疏数据 难以填充缺失值 热独编码(Onehot-encoding) 通常和大多数线性算法一起使用 稀疏格式对内存友好 大多数方法无法处理缺失或者不可见数据 对于没有大小区分的类别特征，可以使用Oneshot-encoding 哈希编码(Hash encoding) 固定长度的oneHot-encoding 避免数据过度稀疏 碰撞通常会降低准确率，但是也能提高 可以很好地处理新变量 Label encoding 给每个类别变量一个唯一的数字ID 对于非线性树模型比较有用 不会增加数据维度 随机化映射 car_var-&gt;num_id的映射，在训练，平均，对精度能有小提升(小提升) 计数编码(Count encoding) 在训练集中用他们的统计量替代类别特征 对于线性和非线性算法都有用 对于异常值很敏感 增加log转换，对于统计量可能更好 用1替代无法观测数据 可能会造成冲突，同一个编码，不同的变量值 计数排名编码(LabelCount encoding) 通过统计量给类别特征排序 对于线性和非线性算法都有用 对异常值不敏感 不会给同一个变量不同的值 Category Embedding 使用神经网络对类别特征创建一个稠密的embeddings，找到不同类别特征关联性 NaN encoding： 给NaN 值一个明确的编码，而不是忽略 NaN里面也包含信息 小心过拟合 仅仅当训练集和测试集NaN都是同样的原因造成，或者本地验证结果证明NaN包含信息(即这种方法是有效果的) 数字特征 对于算法更可读 很容易填补缺失值 截断(Rounding) 对数字变量四舍五入 有损压缩，保留数据最重要的特征 有时候精度太准确只是噪声 四舍五入后的变量能够当成类别特征处理 在四舍五入之前可以进行log转换 分桶(Binning) 对数据进行分桶，并且赋予一个分桶ID 分桶能够使用分位数或者模型来找到最佳分桶 能够很好地处理超出训练集范围的数据 分桶的作用，当一个特征的数值比较大，但是模型对数值比较敏感的时候，最好的方法是分桶，将数值变量分配到一个桶里。 缩放(Scaling) 将数字变量缩放到一个明确的范围 Standard(Z) Scaling MinMax Scaling Root Scaling Log Scaling 原因：因为部分模型例如线性回归、逻辑回归等对于数据输入比较敏感将数值缩放到一个维度， 标准化Standarisation和归一化Normalisation 归一化的原因 当变量维度不同的时候，对模型产生的作用不一致，这时候需要归一化 神经网络在特征缩放之后训练速度更快 特征缩放的时机 KNN K-means NN 树模型不需要特征缩放 填充(Imputation) 对缺失值进行填充 硬编码能够和填充结合起来 Mean：最常见的 Median：对于异常值更有鲁棒性 Igonre：忽略，后面在处理或者不处理？ 使用模型对结果进行预测，然后填充 交叉(Interaction) 对数字变量之间的相互影响进行编码 尝试加减乘除 使用特征选择：统计测试、或者已经训练好的特征重要性 忽略主观意愿和奇怪的关联能够有明显提升 高斯转换 一些机器学习模型假设数据是正态分布的，例如linear和logistic回归 高斯分布能够帮助机器学习下模型表现得更好 集中不同的转换 Logarithmic Transformation Reciprocal Transformation Square Root Transformation Exponential Transformation Boxcox Transformation 相关问题： 1.为什么要做对数变换 https://www.zhihu.com/question/22012482 https://tianchi.aliyun.com/notebook-ai/detail?postId=62338 异常值(Outlier)1、异常点是否需要清除，需要结合具体业务 2、异常点的定义 高于1.5倍的第三分位数和第一分位数之差 与均值的差小于三倍的标准差 3、异常值存在的原因 数据多样性 测量错误 4、异常点的影响 在统计分析的时候造成多种问题 对于数据均值和标准差影响比较大 5、找出异常值 IQR z score Scatter plots Box plot 时间特征 日起相关特征，节假日之类 lag feature，根据业务决定选择lag1、lag2、。。。。 检查lag函数重要性两种方法 ACF，Autocorrelation Function PACF, Partial Autocorrelation Function rolling windows 滑动窗口 Expanding windows 空间特征 欧氏距离 球面距离 曼哈顿距离 geohash 文本特征N-GramN-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。 每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。 该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。 词袋模型词袋模型可以理解为将文本拆成一个个的词语，然后用一个个词作为字典来表达文本 ，字典中的词没有特定的顺序，也舍弃了句子总体结构。其中TF-IDF是一种表示方式 TF-IDF用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度 TF-IDF的核心思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上是：TF*IDF。TF是词频(Term Frequency)，指的是特定词语在该文件中出现的频率。 tf_{i,j}=\frac{n_{i,j}}{\sum_{k}n_{n,j}}其中$tf_{i,j}$表示词i出现在文章j中的次数，分母表示文章j中出现档次数量总和。 IDF是逆文本频率指数(Inverse Document Frequency)。IDF的意思是在文档集D中，包含词i的数量越少，词i对于文档集越重要，此时更好区分。 idf_i=log\frac{|D|}{|{j:t_i\in d_j}|}$|D|$表示所有文档数量，文档集中出现此$t_i$的文档数。 余弦相似度Jaccard相似度定义：两个文档中相交的单词个数除以两个文档单词总数之和 J(d_1,d_2)=\frac{|d_1\cap d2|}{|d1 \cup d2|}Jaccard距离 d_j(d_1,d_2)=1-J(d_1,d_2)=\frac{|d_1\cup d2| - |d_1 \cap d_2|}{|d_1\cup d_2|}编辑距离衡量两个字符串相似度的指标，指的是两个字符串有一个字符串转成另外一个字符串需要的最少编辑操作(插入、删除、替换)次数 参考资料 1、https://www.kaggle.com/pavansanagapati/feature-engineering-a-comprehensive-tutorial/notebook 2、美团机器学习实践 3、词袋模型 4、百面机器学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>特征工程</tag>
        <tag>数据挖掘</tag>
        <tag>特征构造</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word embedding作用及用法]]></title>
    <url>%2F2020%2F06%2F07%2Fword-embedding%E4%BD%9C%E7%94%A8%E5%8F%8A%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[关于word embedding总结词向量表示词语的向量都可以称为词向量，one shot向量和distributed向量都可以表示为词向量热编码表示(one shot)优点 解决了分类器不好解决离散数据的问题， 起到了扩充特征的作用 缺点： 首先，它是一个词袋模型，不考虑词与词之间的顺序(文本中词的顺序信息非常重要) 其次假设词与词之间是独立的(大多数情况下，词与词之间是相互影响的) 最后得到的特征是系数的 分布式表示核心思想 通过训练将某种语言中的每一个词映射成一个固定长度的短向量（当然这里的“短”是相对于 one-hot representation 的“长”而言的），将所有这些向量放在一起形成一个词向量空间，而每一向量则为该空间中的一个点，在这个空间上引入“距离”，则可以根据词之间的距离来判断它们之间的（词法、语义上的）相似性了。 基于矩阵的分布表示基于聚类的分布表示基于神经网络的分布表示1、基本概念 基于神经网络的分布表示一般称为word embedding(词嵌入)或者distributed representation 2、word2vec 两种方式 CBOW:输入一个词上下文，输出这个词 Skip-Gram：输入一个词输出这个词的上下文 embedding layer：和word2vec一样 embedding layer和word2vec 参考资料1、DeepNLP的表示学习·词嵌入来龙去脉·深度学习（Deep Learning）·自然语言处理（NLP）·表示（Representation） 2、word2vec 中的数学原理详解 3、秒懂词向量本质]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>NLP</tag>
        <tag>词向量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lightGBM论文总结]]></title>
    <url>%2F2020%2F03%2F10%2Fxgboost%E5%92%8ClightGBM%2F</url>
    <content type="text"><![CDATA[LightGBM提出动机为了解决GBDT在海量数据中遇到的问题，让GBDT算法更好的适用于工业实践。1、XGBoost的缺点 需要保存特征值和排序结果，还需要保存排序的索引 每次分裂一个点的时候，都需要计算收益 对cache优化不友好，容易造成cache miss 2、LightGBM的优化 单边梯度采样GOSS 直方图算法 互斥特征捆绑算法 Leaf-Wise分裂算法 类别特征最有分裂 并行学习优化 cache命中率优化 2. 数据原理2.1.基于直方图的算法对于XGBoost，其实现是预排序算法，LiggtGBM是基于直方图的算法。 直方图算法计算过程 遍历每一个叶子结点的每一个特征 为每一个特征创建一个直方图，将样本的梯度($g_i$)之和和样本数$n$保存到bin中 然后遍历所有的bin，以当前的bin作为分裂点，然后计算分裂后的左右节点梯度和节点数目 通过直方图加速法算出左右节点的梯度和$S_L$和$S_R$，已经bin的数量 计算分裂后的收益$loss=\frac{S_L^2}{n_L}+\frac{S_R^2}{n_R}-\frac{S_P^2}{n_p}$ 其实，思想和xgb差不多，在选择分裂点的时候，xgb用的是预排序算法，lgb用的是梯度直方图。 然后在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data#feature)优化到O(k#features)。 直方图离散化优点： 占用内存更小，相对于XGBoost预排序算法，无需存储特征值和排序索引 计算代价更小，预排序算法需要每遍历一个特征值，就计算一次收益，而直方图算法只用计算K次，时间复杂度有O(data feature)下降到O(k feature) 当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。 2.2 单边梯度采样GOSS对于GBDT的数据，梯度越大，说明训练误差越大，这样的样本对模型的提升也越大(adaboost思想)，因此GOSS的算法的思想是保留梯度交大的样本，然后在剩余梯度较小的样本中进行采样。不直接丢掉梯度较小的样本数据的原因是会影响数据总体分布。 具体流程 首先将分裂的特征按照绝对值大小进行排序 选择梯度最大的a%的数据 在剩余的数据中随机挑选b%个数据 然后对于梯度较小的数据，乘以1个常数$\frac{1-a}{b}$ 最后将挑选出来的数据进行合并，计算信息增益 2.3 互斥特征捆绑算法互斥捆绑算法的目的是为了减少特征维度，因为实际任务中，特征一般是高维稀疏的。 对于完全互斥的特征，可以将其捆绑起来，例如one-shot产生的特征，捆绑之后不会造成信息丢失。 对于不完全互斥的特征，存在部分情况下两个特征都为非0值，可以使用冲突比率（同时不为0的样本数之和/所有样本数）对不互斥程度进行衡量，当小于一定值时，可以将两个不完全互斥的特征捆绑。 对于特征捆绑，有两个问题 1、如何确定哪些特征需要绑在一起 2、如何构建绑定后的特征 对于第一个问题，确定那些问题需要绑定，LightGBM的做法如下 1、构建一个无向加权图，顶点表示特征，边的权值大小表示冲突比率 2、基于特征在图中的度数进行降序排序 3、遍历每个捆绑特征，检查捆绑之后特征数是否小于最大冲突数 冲突数小于K，将该特征添加到捆绑 冲突数大于K，创建新的捆绑特征 对于第二个问题，如何构建绑定后的特征，关键在于如何从绑定后的特征识别出原始特征中的值。基于直方图算法存储的是离散的箱子，而不是连续的特征值。LightGBM是基于特征从属于不同的箱子来构建捆绑特征的。假设特征A的原始特征取值空间为[0,10),特征B的取值空间为[0,20)，当 此时可以在特征B的区间上加上偏置10，此时B的取值空间为[10,20)，而AB绑定后的特征取值空间为[0,30)。 EFB算法可以将很多互斥稀疏特征捆绑成少量稠密特征，避免针对特征值0的不必要计算。虽然可以优化基于直方图的算法，使用一张表保存每个特征非0取值，然后通过扫描这张表来构建直方图，这样时间复杂度就从原来的O(data)变成了O(no_zero_data)，缺点在于需要额外的算力和空间保存和更新这张表。LGB将其作为辅助功能。 2.4 Leaf-Wise分裂算法 2.5 类别特征最优分裂这部分没怎么看懂，参考的 离散特征建立直方图的过程：统计该特征下每一种离散值出现的次数，并从高到低排序，并过滤掉出现次数较少的特征值, 然后为每一个特征值，建立一个bin容器, 对于在bin容器内出现次数较少的特征值直接过滤掉，不建立bin容器。 计算分裂阈值的过程： 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点; 对于bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算(公式如下: 该bin容器下所有样本的一阶梯度之和/该bin容器下所有样本的二阶梯度之和 + 正则项(参数cat_smooth)，这里为什么不是label的均值呢？其实上例中只是为了便于理解，只针对了学习一棵树且是回归问题的情况， 这时候一阶导数是Y, 二阶导数是1)， 3 工程优化3.1 并行学习优化LightGBM 提供以下并行学习优化算法： 特征并行适用于数据量比较少，feature比较多 传统的特征并行算法旨在于在并行化决策树中的“ Find Best Split.主要流程如下: 垂直划分数据（不同的机器有不同的特征集） 在本地特征集寻找最佳划分点 {特征, 阈值} 本地进行各个划分的通信整合并得到最佳划分 以最佳划分方法对数据进行划分，并将数据划分结果传递给其他线程 其他线程对接受到的数据进一步划分 然而，该特征并行算法在数据量很大时仍然存在计算上的局限。因此，建议在数据量很大时使用数据并行。 数据并行适用于大数据，feature比较少 数据并行旨在于并行化整个决策学习过程。数据并行的主要流程如下： 水平划分数据 线程以本地数据构建本地直方图 将本地直方图整合成全局整合图 在全局直方图中寻找最佳划分，然后执行此划分 数据并行的缺点 机器的通讯开销大约为 “O(#machine #feature #bin)” 。 如果使用集成的通讯算法（例如， “All Reduce”等），通讯开销大约为 “O(2 #feature #bin)”[8] 。 投票并行大数据并且feature比较多 基于投票的并行是对于数据并行的优化，主要分为两步： 通过本地数据，找到本地top k的特征 利用投票筛选出可能是全局最优点的特征 合并直方图时，只合并被选出来的部分 3.2 Cache命中率优化预排序算法： 不同的特征访的梯度顺序不同 对于索引表的访问，pre_sort使用了行号和叶子节点的索引表 都是随机访问，容易造成cache miss lightGBM对直方图优化： 梯度直方图不需要对梯度进行排序 直方图算法不需要数据到叶子id的索引表 4 XGBoost和LightGBM区别 xgboost是预排序算法，lightGBM是直方图算法。 分裂方式，xgb是level-wise，lgb是Leaf-Wise lgb支持类别特征 采用了单边梯度采样和互信息捆绑进行优化 并行化，feature在节点进行分裂的时候采用了多线程并行化，而lgb采用了特征并行、数据并行、投票并行 基于分裂算法的不同，lgb对cache命中更加高效 参考1、https://cloud.tencent.com/developer/article/1528372 2、https://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog 3、https://www.biaodianfu.com/lightgbm.html 4、https://www.zhihu.com/question/266195966 5、https://lightgbm.apachecn.org/#/docs/4 6、直方图算法深入理解]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>Xgboost</tag>
        <tag>lightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XGBoost原理总结]]></title>
    <url>%2F2020%2F03%2F09%2FXGBoost%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Xgboost, 是GBDT的一种实现方式，并且xgboost做了一些改进和优化。1. 原理1.1 优化目标函数对于GBDT方法，都是基模型组成的加法公式。 \hat y_i = \sum_{i=1}^kf_t(x_i)\tag{1}其中$f_k$为基模型,$y_i$表示第$i$个样本预测值。正则化损失函数 \zeta^t=\sum_{i=1}^n l(y_i,\hat y_i^{(t-1)})+\Omega(f_t)\tag{2}对于损失函数（2）进行二阶展开有： \zeta^{(t)} \approx \sum_{i=1}^n[l(y_i,\hat y_i) +g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t) \\ where\quad \Omega(f)=\gamma T+\frac{1}{2}\lambda||w||^2\tag{3}对于损失函数，xgboost在处理的时候进行了二阶展开，其中$g_i=\frac{\partial l(y_i,\hat y_i^{(t-1)})}{\partial \hat y_i^{(t-1)}}$, $h_i=\frac{\partial ^2l(y_i,\hat y_i^{(t-1)})}{\partial (\hat y_i^{(t-1)})^2}$。其中$g_i$和$h_i$分别对应一阶倒和二阶倒数，正则项$T$表示叶子节点数目，$w$表示叶子的分数。$\gamma$空值叶子节点的个数，保证叶子节点不会过多分裂，而$\lambda$空值叶子结点的分值，避免分值过大造成过拟合。 对于第$t$步而言，前面的$t-1$步已经固定，因此有一阶、二阶梯度$g_i$和$h_i$为一个常数。因此目标函数可以化简为 \hat \zeta^t=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)\tag{4}定义$I_j=\{x|q(x_i)=j\}$,表示为叶子结点$j$中的样本。所以上式(3)可以重写为 \hat\zeta^{(t-1)}=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if^2_t(x_i)]+\gamma T + \frac{1}{2}\lambda \sum_{j=1^T}w_j^2\tag{5} =\sum_{j=1}^T[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\gamma T \tag{6}这里其实进行了一个转换，对于公式5而言，计算的损失函数是将所有数据得到损失函数。对于决策树，样本最终会落到叶子结点，因此公式6是通过叶子节点求损失值。 对于固定结构的$q(x)$，即改树节点时固定的，可以计算叶子结点$j$的最优权重$w_j^*$ w_j^*=-\frac{2\sum_{i\in I_j}g_i}{\sum_{i\in I_j}h_i+\lambda}将结果带入上式6有 \hat \zeta^{(t)}=-\frac{1}{2} \sum_{j=1}^T\frac{(\sum_{i\in I_j}g_i)^2}{\sum_{i\in I_j}h_i+\lambda}\tag{7}定$G_j=\sum_{i\in I_j}g_i$,$H_j=\sum_{i\in I_j}h_i$，则有 w_j^*=-\frac{G_j}{H_j+\lambda}将上式带入公式7化简有 \zeta^{(t)}=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}\tag{8}对于Xgboost使用泰勒展开的原因是因为想统一损失函数的形式，方便自定义损失函数。 1.2最佳切分点算法xgboost支持两种实现，贪心算法和近似算法。sklearn中GBDT是贪心算法 1）贪心算法，和GBDT一样，暴力枚举 1、对于所有叶子节点枚举可用的特征，并且将特征值按照升序排序 2、计算节点分裂时候的收益 3、选择收益做大的节点和特征进行分裂 4、重复1，直到分裂结束 关键点在于对收益的计算 假设某一节点完成分裂，在分裂前，其目标函数为 L(y,\hat y_i)=-\frac{1}{2}[\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]+\gamma\tag{9}分裂后的目标函数为 L=-\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}]+2\gamma\tag{10}所以分裂一个节点的收益可以从用式（9）-（10） Gain=-\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\gammaG表示所有叶子节点的梯度 2)近似算法 作用在于选择，当数据量比较大，无法全部读入内存时，给出近似最优解。对比贪心算法，可能在精度上有所缺失，但是提升了速度，降低了内存消耗。 该算法的核心思想是根据特征分布的分位数提出候选点，然后将特征映射到候选划分的桶之中，然后统计桶中的聚合信息(指的前面的$g$和$h$)，找到所有区间最佳分裂点。 1、对于特征k根据分位数找到候选集合 2、将样本映射到改候选集合对应的分区桶中 该算法有两种变体，区别在于何时剔除候选点： Global：在初始阶段就给出所有候选节点，并且在后续分裂中使用相同的分裂节点。 Local：每次分裂重新提出候选节点 分位图 加权分位图： 由于前面我们知道目标函数为 L=\sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)\tag{11}由于$g_i$和$h_i$是有上一轮迭代得到，因此都是常数，所以上式可以变形为： L\approx\sum_{i=1}^n\frac{1}{2}h_i[(f_t(x_i)+\frac{g_i}{h_i})^2]+\Omega(f_t)+C \\C=-\frac{g_i^2}{h_i}\tag{12}这样损失函数就变成了加权的形式，因此对于每个样本，其实权值是不等的，所以采用加权分位图。 1.3 稀疏感知分裂在实际问题中，通常输入数据都是稀疏的，造成稀疏的原因有： 数据缺失 一些统计量常常为0 特征工程的结果，如one-shot 稀疏感知算法的目的是给每个节点一个默认的分裂方向，其思想非常简单，就是分别计算缺失值样本分裂到左边或者右边是的收益，选择收益大的一个分支作为最优缺省值方向 2. 工程优化2.1 块结构设计树学习中最耗时的部分是数据排序。为了减少排序的成本，我们提出将数据存储在内存单元中，称之为block。每个block中的数据每列根据特征取值排序，并以压缩列（CSC）格式储存。这种输入数据布局只需要在训练前计算一次，可以在后续迭代中重复使用。 每个块包含一个或者多个已经排好序的特征 缺失值将不在进行排序 每个特征值都会存储样本梯度统计值索引 因为每个特征都是独立存放，因此在选择特征进行分裂的时候可以分布式实现 2.2 缓存方法优化算法是通过行索引提取梯度统计量，但是在排序之后就会乱掉，不能够直接访问。并且当统计量没法放进CPU缓存是，会导致访问失败，因此xgb给每个线程分配一个内部缓冲区。 2.3 核外快计算方式对于数据量比较大的数据，没有办法存储到内存，可以考虑部分读取，将数据存储到硬盘，但是硬盘读取会占用大量时间 XGBoost采用两种方式降低硬盘读取开销 1、块压缩：对Block进行案列压缩，并且在读取时解压 2、块拆分：将每个块存储到不同的磁盘，然后从多个磁盘读取增加吞吐量。 3. GBDT和XGBoost区别 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。 Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率） 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。 调参技巧略，直接看API就行了。。。。懒得总结了 参考资料 [1].https://www.zhihu.com/question/41354392/answer/98658997 [2].https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ [3].行抽样、列抽样]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop数据倾斜及解决办法]]></title>
    <url>%2F2019%2F12%2F15%2Fhadoop%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>hadoop</category>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于决策树总结]]></title>
    <url>%2F2019%2F11%2F04%2F%E5%85%B3%E4%BA%8E%E5%86%B3%E7%AD%96%E6%A0%91%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[决策树处理连续值]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度提升树]]></title>
    <url>%2F2019%2F10%2F10%2FGBDT%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[GBDT主要由三个概念组成：Regression Decistion Tree（即DT)，Gradient Boosting（即GB)，Shrinkage（算法的一个重要演进分枝，目前大部分源码都按该版本实现）。搞定这三个概念后就能明白GBDT是如何工作的，要继续理解它如何用于搜索排序则需要额外理解RankNet概念，之后便功德圆满。下文将逐个碎片介绍，最终把整张图拼出来。 加法模型对于算法模型而言，一个性能弱的算法模型可能很难得到很好的效果，加法模型的思想是将性能较弱的模型通过加权得到一个性能较强的模型。形如 f(x)=\sum_{m=1}^{M}\beta_m b(x;\gamma_m)\tag{1}其中$b(x;y_m)$表示基函数，$\gamma_m$表示基函数系数，$\beta_m$表示基函数系数。 前向分布算法在给定训练集的情况下以及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$即为最小化损失函数的问题： \min\limits_{\beta_m,\gamma_m}\sum_{i=1}^N L(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))\tag{2}前向分步算法的思想：加法模型是不同模型的组合，因此从前向后每次学习一个基函数和基函数系数来逐步优化目标函数$(1)$,从而降低复杂度。 计算流程： (1).初始化第一个基函数$f_0(x)$ (2)对于$m=1,2,3,…,M$，极小化损失函数 (\beta_m,\gamma_m)=\arg \min\limits_{\beta,\gamma}+\beta_mb(x;\gamma_m)\tag{3}得到参数$\beta_m,\gamma_m$ (3) 更新加法模型 f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)\tag{4}(4)得到加法模型 f(x)=\sum_{i=1}^M\beta_mb(x;\gamma_m)\tag{5}GBDT梯度提升模型提升树算法提升方法可以总结为加法模型与前向分布算法，以决策树为基函数的模型成为提升树，无论是分类问题还是回归问题，都是基于回归树(这点和统计学系方法里面不一样)，提升树算法则是采用前向分步算法来更新加法模型。对于提升树，基函数变为决策树，所以加法模型为 f_m(x)=\sum_{i=1}^MT(x;w_m)\tag{6}其中$M$为决策树的个数，$w$为决策树的参数，$T$表示决策树。 初始化第一棵决策树，第$m$部的模型为 f_m(x)=f_{m-1}(x)+T(x;w)\tag{7}通过最小化损失函数确定下一棵决策树的参数$w_m$ \arg\min\limits_{w_m}\sum_{i=1}^NL(y_i,f_{m-1}+T(x_i,w_m))当采用平方误差时 L(y,f(x))=(y-f(x))^2\tag{8}损失函数变为 L(y)=(y-f_{m-1}(x)-T(x;w_m))^2 =[r-T(x;w_m)]^2\tag{9}其中残差$r=y-f_{m-1}(x)$，所以最后的目的就是为了是$T(x;w_m)$的值更加接近残差，从而达到最小化损失函数的作用。 回归问题提升树1.计算出第一颗树第一棵提升树 f_0(x)=\arg \min\limits\sum_{i=1}^NL(y_i,c)\tag{10}2.得到提升树的残差 r_{mi}=y_i-f_{m-1}(x_i), i=1,2,3....,N\tag{11}3.通过拟合残茶学习回归树，得到$T_m(x;w_m)$ 4.更新提升树 f_m(x)=f_{m-1}(x)+T(x;w_m)\tag{12}梯度提升梯度提升本质其实是利用梯度下降算法来对前向分步算法进行优化求解的方法。其关键是利用损失函数负梯度在当前模型的值作为残差的近似值，进行一个拟合。 r_{mi}=-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x_i)=f_m(x_i) }\tag{13}利用负梯度代替残差的原因是因为只有在损失函数为平方差的时候，梯度才等于残差，但是当损失函数比较复杂的时候，此时梯度是不等于残差的。 对于特征的选择和回归树一样，同样是遍历所有特征找到最佳切分点。 回归例子可以参见统计学习方法。 GBDT用于分类和回归的区别前面主要将的是GBDT的思想，利用残差不断的拟合，直到最后接近目标。但是对于对于分类和回归任务的处理，主要有以下几个方面不一样。 特征选择1、分裂节点的评价标准不同 对于回归类问题，分裂节点的时候主要评价方式为 (1)平方误差 L(x,c)=min \sum_{i=1}^m\sum_{j\in R_i}(x_j-c_i)^2将特征划分为m个不同的区域$R_i$，然后求出每个区域的平方误差求和，平方误差和最小的特征和切分点。 (2)绝地值误差 L(x,c)=min \sum_{i=1}^m\sum_{j\in R_i}|x_j-c_i|(3)friedman_mse：费尔德曼均方误差，改进后的均方误差，一般能够达到比较好的效果 对于分类问题，其节点分类的评价方式为 (1)信息熵(entropy) H(x)=-\sum_{i=1}^np_i\log p_i(2)gini,基尼系数(信息增益) g(D,A)=H(D)-H(D|A)详细计算过程见统计学习方法。 损失函数在介绍分类的原理之前首先要了解一下对数损失函数 L(y,P(Y|X))=log P(Y|X)\tag{14}对于分类任务，GBDT是结合回归加分类模型计算每种分类的概率，对于二分类，采用的是logistic进行分类 P(Y=1|X)=\frac{1}{1+exp(-\sum_{i=1}^Mf_i(x))}\tag{15} P(Y=0|X)=\frac{1}{1+exp(\sum_{i=1}^Mf_i(x))}\tag{16}令$h_\theta(x)=\frac{1}{1+exp(-\sum_{i=1}^Mf_i(x))}$ 所以有 P(Y=1|X)=h_\theta(x)\tag{17} P(Y=0|X)=1-h_\theta(x)\tag{18} P(Y|X)=h_\theta(x)^{y_i}(1-h_\theta(x))^{1-y_i}损失函数为 J_\theta(x)=-\sum_{i=1}^N [y_ilogh_\theta(x)+(1-y_i)log(1-h_\theta(x))]\tag{19}所以经过计算有 \frac{\partial J}{\partial h_\theta(x)}=y-\hat y对于多分类问题 损失函数为交叉熵 L(y,p(y|x))=-\sum_{i=1}^M y_ilog {p_i}\tag{20}其中$i$表示所属类别，$M$表示分类树,$p_i$表示属于$i$的概率 并且有 p(y=i|x)=\frac{exp(F_i(x))}{\sum_{i=1}^Mexp(F_i(x))}\tag{21}同样求梯度有 r_{mi}=-\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}|_{f(x_i)=f_{m-1}(x_i)}回归损失函数 (1)平方损失函数 L(y,x)=\sum_{i=1}^n(y_i-f(x_i))^2(2)绝对值损失函数 L(y,x)=\sum_{i=1}^n|y_i-f(x_i)|(3)huber损失函数 L(y)=\left\{ \begin{array}{rcl} \frac{1}{2}(y-f(x))^2 & & {|y-f(x)|\leq\delta}\\ \delta*|y-f(x)-\frac{1}{2}\delta| & & {|y-f(x)|>\delta} \end{array} \right.GBDT的正则化和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。 （1）第一种是和Adaboost类似的正则化项，即步长（learning rate）。定义为ν,对于前面的弱学习器的迭代 f_k(x)=f_{k-1}(x)+h_k(x) 如果我们加上了正则化项，则有 f_k(x)=f_{k-1}(x)+v\cdot h_k(x) ν的取值范围为0&lt;ν≤10。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。 （2）第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。 使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。（注：这一点没明白。。） （3）第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树章节里我们已经讲过，这里就不重复了。 调参经验分类sklearn.ensemble.`RandomForestClassifier Parameters n_estimators ：树的个数，迭代次数 The number of trees in the forest.Changed in version 0.22: The default value of n_estimators changed from 10 to 100 in 0.22. criterion： 叶子结点分裂的方式，默认的是gini和entropy max_depth：树的深度，默认为空，会一直分裂，直到无法继续分裂 min_samples_split： 分裂一个节点所需要的最小样本 int：表示样本数 float 表示的百分比 min_samples_leaf：保持一颗叶子结点所需要的样本数，该参数能够对模型进行平滑，特别在回归任务中。int和float和min_samples_split一样。 min_weight_fraction_leaf：叶子结点所有权重和的最小值，如果分布相差很大或者有很多缺失值，可以引入该参数 max_features：当考虑最佳分割点是考虑的特征数。 如果是float型，表示的是百分比 如果是’auto’ or ‘log2’，表示sqrt(n_features) 如果是log2, 表示log2(n_features) max_leaf_nodes : 最大叶子结点，用于防止过拟合 min_impurity_split：早停的阈值，如果一个节点的不纯度高于该值，则分裂，否则为叶子结点 Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split will change from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead. bootstrap： 是否使用bootstrap采样，为否表示使用整个数据集 oob_score：袋外精度来泛化 Whether to use out-of-bag samples to estimate the generalization accuracy. class_weight：类别权重，用于样本分布不均衡时使用 ‘’dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None） 格式为{class_label: weight} ，例如 {0: 1, 1: 1} ’balanced‘模式下会自动调整权值，根据训练数据中类别出现频率， n_samples/(n_class *np.bincount()) ‘balanced_subsample’和balanced一样，区别在于才用的boostrap max_samples：从训练集中取出的每个样本量 None：表示使用所有样本 如果为int 表示为该值 float表示 百分比 class sklearn.ensemble.``GradientBoostingRegressor loss ：损失函数，默认为ls ‘ls’ 平方损失函数，损失函数为$L(y)=(y-f(x))^2$ ‘lad’，绝对值 ,损失函数 $L(y)=|y-f(x)|$ ‘huber’： 两者的结合 L(y)=\left\{ \begin{array}{rcl} \frac{1}{2}(y-f(x))^2 & & {|y-f(x)|\leq\delta}\\ \delta*|y-f(x)-\frac{1}{2}\delta| & & {|y-f(x)|>\delta} \end{array} \right. subsample：子采样比例，子采样会减少方差，增大偏差 criterion： 衡量节点分裂质量的指标 friedman_mse, ‘mse’ ’mae‘ New in version 0.18. min_samples_split: 和分类一样 tol：学习率 参考资料 [1]https://zhuanlan.zhihu.com/p/86281279 [2].统计学习方法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas的基本用法]]></title>
    <url>%2F2019%2F10%2F05%2Fpandas%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib的基本用法]]></title>
    <url>%2F2019%2F10%2F04%2FMatplotlib%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Matplotlib的基本用法简单的折线图plt.plot(x,y, fortmat_string)作用是定义画图的样式x,y表示横纵左表， format可以定义画图格式12345678#导入Matploylib库from matplotlib import pyplot as plt %matplotlib inline #画布上画图plt.plot([1,2,3,4], [1,2,3,4], 'b', linewidth=2)plt.plot([1,2,3,4], [1,4,9,16], 'r', linewidth=2)#在画布上显示plt.show() 添加标题标签及图的样式123456789101112from matplotlib import pyplot as plt %matplotlib inline x = [5,2,7]y = [2,16,4]plt.plot(x,y)# 图片的标题plt.title('Image Title')# 坐标轴Y轴plt.ylabel('Y axis')# 坐标轴X轴plt.xlabel('X axis')plt.show() 12345678910111213141516from matplotlib import pyplot as pltfrom matplotlib import style style.use('ggplot')x = [5,8,10]y = [12,16,6]x2 = [6,9,11]y2 = [6,15,7]plt.plot(x,y,'g',label='line one', linewidth=5)plt.plot(x2,y2,'r',label='line two',linewidth=5)plt.title('Epic Info')plt.ylabel('Y axis')plt.xlabel('X axis')#设置图例位置plt.legend()plt.grid(True,color='k')plt.show() 直方图pyplot.bar(left, height, alpha=1, width=0.8, color=, edgecolor=, label=, lw=3) 画一个柱状图 参数 left： x轴的位置序列，一般采用arange函数产生一个序列 height: y轴的数值序列，也就是柱状图的高度，即我们需要展示的数据 alpha： 透明度 width: 柱状图的宽度 color or facecolor: 柱状图的填充颜色 edgecolor: 图形边缘颜色 label: 每个图像代表的意思 linewidth or linewidths or lw：边缘or线的宽度 12345678from matplotlib import pyplot as plt plt.bar([0.25,1.25,2.25,3.25,4.25],[50,40,70,80,20],label="BMW", color='b', width=.5)plt.bar([.75,1.75,2.75,3.75,4.75],[80,20,20,50,60],label="Audi", color='r',width=.5)plt.legend()plt.xlabel('Days')plt.ylabel('Distance (kms)')plt.title('Information')plt.show() 频率图matplotlib.pyplot.hist(x, bins=10, range=None, normed=False, weights=None, cumulative=False, bottom=None, histtype=u’bar’, align=u’mid’, orientation=u’vertical’, rwidth=None, log=False, color=None, label=None, stacked=False) 统计每个区间出现的频率 参数 x：直方图统计的数据 bins: 指定统计的间隔，如bins=10时表示以10为一个区间 color: 柱状图的颜色 histtype: 可选{‘bar’, ‘barstacked’,’step’, ‘stepfilled’}之一 density: 显示频率 stacked: 是否显示堆叠柱状图 12345678import matplotlib.pyplot as pltpopulation_age = [22,55,62,45,21,22,34,42,42,4,2,102,95,85,55,110,120,70,65,55,111,115,80,75,65,54,44,43,42,48]bins = [0,10,20,30,40,50,60,70,80,90,100]plt.hist(population_age, bins=10, histtype='bar', color='b', rwidth=0.8)plt.xlabel('age groups')plt.ylabel('Number of people')plt.title('Histogram')plt.show() 散点图12345678910111213import matplotlib.pyplot as pltx = [1,1.5,2,2.5,3,3.5,3.6]y = [7.5,8,8.5,9,9.5,10,10.5] x1=[8,8.5,9,9.5,10,10.5,11]y1=[3,3.5,3.7,4,4.5,5,5.2] # scatter表示画散点图plt.scatter(x,y, label='high income low saving',color='r')plt.scatter(x1,y1,label='low income high savings',color='b')plt.xlabel('saving*100')plt.ylabel('income*1000')plt.title('Scatter Plot')plt.legend()plt.show() 堆叠图matplotlib.pyplot.stackplot(x, args, labels=(), colors=None, baseline=’zero’, data=None, *kwargs) 画堆叠图，主要有三个参数 x:需要画堆叠图的数值 laebl: 堆叠图中折现的标签 colors: 设置折线图的颜色 1234567891011sleeping =[7,8,6,11,7]eating = [2,3,4,3,2]working =[7,8,7,2,2]playing = [8,5,7,8,13] labels = ['Sleeping', 'Eating', 'Working', 'Playing']plt.stackplot(days, sleeping,eating,working,playing,labels=labels,colors=['m','c','r','k']) plt.xlabel('x')plt.ylabel('y')plt.title('Stack Plot')plt.legend()plt.show() 饼状图123456slices = [7,3,2,13]activities = ['sleeping','eating','working','playing']cols = ['c','m','r','b'] plt.pie(slices, labels=activities, colors=cols, startangle=90, shadow= True, explode=(0,0.1,0,0), autopct='%1.1f%%') plt.title('Pie Plot')plt.show() 多个子图合并plt.subplot(numRows, numCols, plotNum) 将一块画布分为多个区域，将不同图分别放入不同的子图 参数 numRows：指的行数 numCols：指的列数 plotNum：子图的位置 如上面所示的221，表示的是将图分为2 * 2个子图，然后使用第一个位置 子图的位置依次为 123(1,1) (1,2)(2,1) (2,2) 依次对应的位置为1,2,3,4 1234567891011import numpy as npimport matplotlib.pyplot as plt def f(t): return np.exp(-t) * np.cos(2*np.pi*t)t1 = np.arange(0.0, 5.0, 0.1)t2 = np.arange(0.0, 5.0, 0.02)plt.subplot(221)plt.plot(t1, f(t1), 'bo', t2, f(t2))plt.subplot(222)plt.plot(t2, np.cos(2*np.pi*t2))plt.show() pandas与matplotlib结合123import pandas as pdimport numpy as npimport matplotlib.pyplot as plt np.random.rand(nums) 随即产生nums个位于[0,1]的样本 np.random.randn(nums) 随即返回nums个标准正态分布的样本 1plt.plot(np.random.rand(10)) 1[&lt;matplotlib.lines.Line2D at 0x24f3122c2b0&gt;] 设置坐标轴刻度 图名 x轴标签 y轴标签 图例 x轴边界 y轴边界 x轴刻度 y轴刻度 x轴刻度标签 y轴刻度标签 123456789101112131415161718192021df = pd.DataFrame(np.random.rand(10,2),columns=['A','B'])fig = df.plot(figsize=(8,4)) # figsize：创建图表窗口，设置窗口大小plt.title('TITLETITLETITLE') # 图名plt.xlabel('XXXXXX') # x轴标签plt.ylabel('YYYYYY') # y轴标签plt.legend(loc = 'upper right') # 显示图例，loc表示位置plt.xlim([0,12]) # x轴边界plt.ylim([0,1.5]) # y轴边界plt.xticks(range(10)) # 设置x刻度plt.yticks([0,0.2,0.4,0.6,0.8,1.0,1.2]) # 设置y刻度fig.set_xticklabels("%.1f" %i for i in range(10)) # x轴刻度标签fig.set_yticklabels("%.2f" %i for i in [0,0.2,0.4,0.6,0.8,1.0,1.2]) # y轴刻度标签# 这里x轴范围是0-12，但刻度只是0-9，刻度标签使得其显示1位小数 1234567[Text(0, 0, &apos;0.00&apos;), Text(0, 0, &apos;0.20&apos;), Text(0, 0, &apos;0.40&apos;), Text(0, 0, &apos;0.60&apos;), Text(0, 0, &apos;0.80&apos;), Text(0, 0, &apos;1.00&apos;), Text(0, 0, &apos;1.20&apos;)] 修改图标样式pd.Series()作用是产生一个有编号的序列 np.random.randn()产生正太分布的样本，当只有一个参数是，返回n个标准正太分布的结果，当有两个或多个参数时，参数表示对应的维度 np.random.rand() 用法和上面一个函数一样，但是返回的是 np.cumsum()表示将前一行或者前一列加到后面 参数 ​ a：表示传入函数的数据 axi：{0,1}，axi=0时表示行相加，axi=1时表示列相加 12s = pd.Series(np.random.randn(100).cumsum())s.plot(linestyle = '--', marker = '.',color="r",grid=False) dataframe直接画图DataFrame.plot(x=None, y=None, kind=’line’, ax=None, subplots=False, sharex=None, sharey=False, layout=None,figsize=None, use_index=True, title=None, grid=None, legend=True, style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, xerr=None,secondary_y=False, sort_columns=False, **kwds) Parameters:x : label or position, default None#指数据框列的标签或位置参数 y : label or position, default None kind : str‘line’ : line plot (default)#折线图‘bar’ : vertical bar plot#条形图‘barh’ : horizontal bar plot#横向条形图‘hist’ : histogram#柱状图‘box’ : boxplot#箱线图‘kde’ : Kernel Density Estimation plot#Kernel 的密度估计图，主要对柱状图添加Kernel 概率密度线‘density’ : same as ‘kde’‘area’ : area plot#不了解此图‘pie’ : pie plot#饼图‘scatter’ : scatter plot#散点图 需要传入columns方向的索引‘hexbin’ : hexbin plot#不了解此图 ax : matplotlib axes object, default None#子图(axes, 也可以理解成坐标轴) 要在其上进行绘制的matplotlib subplot对象。如果没有设置，则使用当前matplotlib subplot其中，变量和函数通过改变figure和axes中的元素（例如：title,label,点和线等等）一起描述figure和axes，也就是在画布上绘图。 subplots : boolean, default False#判断图片中是否有子图 Make separate subplots for each column sharex : boolean, default True if ax is None else False#如果有子图，子图共x轴刻度，标签 In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in; Be aware, that passing in both an ax and sharex=True will alter all x axis labels for all axis in a figure! sharey : boolean, default False#如果有子图，子图共y轴刻度，标签 In case subplots=True, share y axis and set some y axis labels to invisible layout : tuple (optional)#子图的行列布局 (rows, columns) for the layout of subplots figsize : a tuple (width, height) in inches#图片尺寸大小 use_index : boolean, default True#默认用索引做x轴 Use index as ticks for x axis title : string#图片的标题用字符串 Title to use for the plot grid : boolean, default None (matlab style default)#图片是否有网格 Axis grid lines legend : False/True/’reverse’#子图的图例，添加一个subplot图例(默认为True) Place legend on axis subplots style : list or dict#对每列折线图设置线的类型 matplotlib line style per column logx : boolean, default False#设置x轴刻度是否取对数 Use log scaling on x axis logy : boolean, default False Use log scaling on y axis loglog : boolean, default False#同时设置x，y轴刻度是否取对数 Use log scaling on both x and y axes xticks : sequence#设置x轴刻度值，序列形式（比如列表） Values to use for the xticks yticks : sequence#设置y轴刻度，序列形式（比如列表） Values to use for the yticks xlim : 2-tuple/list#设置坐标轴的范围，列表或元组形式 ylim : 2-tuple/list rot : int, default None#设置轴标签（轴刻度）的显示旋转度数 Rotation for ticks (xticks for vertical, yticks for horizontal plots) fontsize : int, default None#设置轴刻度的字体大小 Font size for xticks and yticks colormap : str or matplotlib colormap object, default None#设置图的区域颜色 Colormap to select colors from. If string, load colormap with that name from matplotlib. colorbar : boolean, optional #图片柱子 If True, plot colorbar (only relevant for ‘scatter’ and ‘hexbin’ plots) position : float Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center) layout : tuple (optional) #布局 (rows, columns) for the layout of the plot table : boolean, Series or DataFrame, default False #如果为正，则选择DataFrame类型的数据并且转换匹配matplotlib的布局。 If True, draw a table using the data in the DataFrame and the data will be transposed to meet matplotlib’s default layout. If a Series or DataFrame is passed, use passed data to draw a table. yerr : DataFrame, Series, array-like, dict and str See Plotting with Error Bars for detail. xerr : same types as yerr. stacked : boolean, default False in line and bar plots, and True in area plot. If True, create stacked plot. sort_columns : boolean, default False # 以字母表顺序绘制各列，默认使用前列顺序 secondary_y : boolean or sequence, default False ##设置第二个y轴（右y轴） Whether to plot on the secondary y-axis If a list/tuple, which columns to plot on secondary y-axis mark_right : boolean, default True When using a secondary_y axis, automatically mark the column labels with “(right)” in the legend kwds : keywords Options to pass to matplotlib plotting method Returns:axes : matplotlib.AxesSubplot or np.array of them 12345# 直接用风格样式设置# 透明度与颜色版# s.plot(style="--.",alpha = 0.8,colormap = 'Reds_r')df = pd.DataFrame(np.random.randn(100, 4),columns=list('ABCD')).cumsum()df.plot(style = '--.',alpha = 0.8,colormap = 'summer_r') 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x24f379a4cc0&gt; 123df = pd.DataFrame(np.random.randn(10,2))df.plot(style = '--o')plt.text(5,0.5,'Hello',fontsize=12) 1Text(5, 0.5, &apos;Hello&apos;) 子图绘制figure对象plt.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=, **kwargs) 作用创建一块画布 num相当于id，如果没有id则递增创建，如果已存在则返回该存在的对象 1234fig1 = plt.figure(num=1,figsize=(8,6))plt.plot(np.random.rand(50).cumsum(),'k--')fig2 = plt.figure(num=2,figsize=(8,6))plt.plot(50-np.random.rand(50).cumsum(),'k--') 1[&lt;matplotlib.lines.Line2D at 0x24f3780f6d8&gt;] 先创建子图然后填充12345678910111213141516171819202122# 先建立子图然后填充图表fig = plt.figure(figsize=(10,6),facecolor = 'gray')# 第一个子图曲线图ax1 = fig.add_subplot(2,2,1)plt.plot(np.random.rand(50).cumsum(),'k--')plt.plot(np.random.randn(50).cumsum(),'b--')# 第二个字图，直方图ax2 = fig.add_subplot(2,2,2)ax2.hist(np.random.rand(50),alpha=0.4)# 第三个饼状图slices = [7,3,2,13]activities = ['sleeping','eating','working','playing']cols = ['c','m','r','b']ax3 = fig.add_subplot(223)ax3.pie(slices, labels=activities, colors=cols, startangle=90, shadow= True, explode=(0,0.1,0,0), autopct='%1.1f%%')# 第四个折线图ax4 = fig.add_subplot(2,2,4) df2 = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd'])ax4.plot(df2,alpha=0.5,linestyle='--',marker='.') 1234[&lt;matplotlib.lines.Line2D at 0x24f38c19940&gt;, &lt;matplotlib.lines.Line2D at 0x24f3905ed68&gt;, &lt;matplotlib.lines.Line2D at 0x24f3905ef28&gt;, &lt;matplotlib.lines.Line2D at 0x24f3906b0f0&gt;] 使用subplots子图数组填充子图12345678910# 创建一个新的figure，并返回一个subplot对象的numpy数组 → plt.subplotfig,axes = plt.subplots(2,3,figsize=(10,4))ts = pd.Series(np.random.randn(1000).cumsum())print(axes, axes.shape, type(axes))# 生成图表对象的数组# 通过数组访问对应的子图ax1 = axes[0,1]ax1.plot(ts) 123456[[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F38EFABA8&gt; &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F38F13BA8&gt; &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F38CA4C88&gt;] [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F38CC8DA0&gt; &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F38CECEB8&gt; &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F38D17320&gt;]] (2, 3) &lt;class &apos;numpy.ndarray&apos;&gt; 1[&lt;matplotlib.lines.Line2D at 0x24f38d39978&gt;] 12345678910# plt.subplots 参数调整fig,axes = plt.subplots(2,2,sharex=True,sharey=True)# sharex,sharey：是否共享x，y刻度for i in range(2): for j in range(2): axes[i,j].hist(np.random.randn(500),color='k',alpha=0.5) # wspace,hspace：用于控制宽度和高度的百分比，比如subplot之间的间距plt.subplots_adjust(wspace=0,hspace=0) 多系列子图绘制plt.plot(): subplots, 是否分别绘制子图,为true的时候表示分开绘制，为false表示在一个图立面绘制 layout：挥之子图矩阵，按顺序填充 subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None) 有六个可选参数来控制子图布局。值均为0~1之间。其中left、bottom、right、top围成的区域就是子图的区域。wspace、hspace分别表示子图之间左右、上下的间距。实际的默认值由matplotlibrc文件控制的。 1234567df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD'))df = df.cumsum()df.plot(style = '--.',alpha = 0.4,grid = True,figsize = (20,8), subplots = True, layout = (2,2), sharex = False)plt.subplots_adjust(wspace=0.1,hspace=0.2) 基本图表绘制Series 与 DataFrame 绘图plt.plot(kind=’line’, ax=None, figsize=None, use_index=True, title=None, grid=None, legend=False,style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, xlim=None, ylim=None,rot=None, fontsize=None, colormap=None, table=False, yerr=None, xerr=None, label=None, secondary_y=False, **kwds) 参数含义： series的index为横坐标 value为纵坐标 kind → line,bar,barh…（折线图，柱状图，柱状图-横…） label → 图例标签，Dataframe格式以列名为label style → 风格字符串，这里包括了linestyle（-），marker（.），color（g） color → 颜色，有color指定时候，以color颜色为准 alpha → 透明度，0-1 use_index → 将索引用为刻度标签，默认为True rot → 旋转刻度标签，0-360 grid → 显示网格，一般直接用plt.grid xlim,ylim → x,y轴界限 xticks,yticks → x,y轴刻度值 figsize → 图像大小 title → 图名 legend → 是否显示图例，一般直接用plt.legend() 12345678910111213141516171819ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000)) # pandas 时间序列ts = ts.cumsum()ts.plot(kind='line', label = "what", style = '--.', color = 'g', alpha = 0.4, use_index = True, rot = 45, grid = True, ylim = [-50,50], yticks = list(range(-50,50,10)), figsize = (8,4), title = 'TEST_TEST', legend = True)# 对网格项进行更加细致的设置#plt.grid(True, linestyle = "--",color = "gray", linewidth = "0.5",axis = 'x') # 网格plt.legend()plt.show() 12345678910111213# subplots → 是否将各个列绘制到不同图表，默认Falsedf = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD')).cumsum()df.plot(kind='line', style = '--.', alpha = 0.4, use_index = True, rot = 45, grid = True, figsize = (8,4), title = 'test', legend = True, subplots = False, colormap = 'Greens') 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x24f32b0f630&gt; 柱状图 plt.bar() x,y参数：x，y值 width：宽度比例 facecolor柱状图里填充的颜色、edgecolor是边框的颜色 left-每个柱x轴左边界,bottom-每个柱y轴下边界 → bottom扩展即可化为甘特图 Gantt Chart align：决定整个bar图分布，默认left表示默认从左边界开始绘制,center会将图绘制在中间位置xerr/yerr ：x/y方向error bar 1234567891011121314151617181920# 创建一个新的figure，并返回一个subplot对象的numpy数组fig,axes = plt.subplots(4,1,figsize = (10,10))s = pd.Series(np.random.randint(0,10,16),index = list('abcdefghijklmnop')) df = pd.DataFrame(np.random.rand(10,3), columns=['a','b','c'])# 单系列柱状图方法一：plt.plot(kind='bar/barh')s.plot(kind='bar',color = 'k',grid = True,alpha = 0.5,ax = axes[0]) # ax参数 → 选择第几个子图# 多系列柱状图df = pd.DataFrame(np.random.rand(10,3), columns=['a','b','c'])df.plot(kind='bar',ax = axes[1],grid = True,colormap='Reds_r')# 多系列堆叠图# stacked → 堆叠df.plot(kind='bar',ax = axes[2],grid = True,colormap='Blues_r',stacked=True) # The bars are positioned at y with the given align. Their dimensions are given by width and height. The horizontal baseline is left (default 0). # https://matplotlib.org/api/_as_gen/matplotlib.pyplot.barh.html?highlight=barh#matplotlib.pyplot.barhdf.plot.barh(ax = axes[3],grid = True,stacked=True,colormap = 'BuGn_r') 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x24f32cf0550&gt; 123456789101112131415plt.figure(figsize=(10,4))x = np.arange(10)y1 = np.random.rand(10)y2 = -np.random.rand(10)plt.bar(x,y1,width = 1,facecolor = 'yellowgreen',edgecolor = 'white',yerr = y1*0.1)plt.bar(x,y2,width = 1,facecolor = 'lightskyblue',edgecolor = 'white',yerr = y2*0.1)for i,j in zip(x,y1): plt.text(i-0.2,j-0.15,'%.2f' % j, color = 'white')for i,j in zip(x,y2): plt.text(i-0.2,j+0.05,'%.2f' % -j, color = 'white')# 给图添加text# zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。 面积图 stacked：是否堆叠，默认情况下，区域图被堆叠 为了产生堆积面积图，每列必须是正值或全部负值！ 当数据有NaN时候，自动填充0，图标签需要清洗掉缺失值 1234567fig,axes = plt.subplots(2,1,figsize = (8,6))df1 = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd'])df2 = pd.DataFrame(np.random.randn(10, 4), columns=['a', 'b', 'c', 'd'])df1.plot.area(colormap = 'Greens_r',alpha = 0.5,ax = axes[0])df2.plot.area(stacked=False,colormap = 'Set2',alpha = 0.5,ax = axes[1]) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x24f3288f668&gt; 填图123456789101112131415161718192021fig,axes = plt.subplots(2,1,figsize = (8,6))x = np.linspace(0, 1, 500)y1 = np.sin(4 * np.pi * x) * np.exp(-5 * x)y2 = -np.sin(4 * np.pi * x) * np.exp(-5 * x)axes[0].fill(x, y1, 'r',alpha=0.5,label='y1')axes[0].fill(x, y2, 'g',alpha=0.5,label='y2')# 对函数与坐标轴之间的区域进行填充，使用fill函数# 也可写成：plt.fill(x, y1, 'r',x, y2, 'g',alpha=0.5)x = np.linspace(0, 5 * np.pi, 1000) y1 = np.sin(x) y2 = np.sin(2 * x) axes[1].fill_between(x, y1, y2, color ='b',alpha=0.5,label='area') # 填充两个函数之间的区域，使用fill_between函数for i in range(2): axes[i].legend() axes[i].grid()# 添加图例、格网 饼图plt.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, hold=None, data=None) 参数含义： 第一个参数：数据 explode：指定每部分的偏移量 labels：标签 colors：颜色 autopct：饼图上的数据标签显示方式 pctdistance：每个饼切片的中心和通过autopct生成的文本开始之间的比例 labeldistance：被画饼标记的直径,默认值：1.1 shadow：阴影 startangle：开始角度 radius：半径 frame：图框 counterclock：指定指针方向，顺时针或者逆时针 1234567891011121314s = pd.Series(3 * np.random.rand(4), index=['a', 'b', 'c', 'd'], name='series')plt.axis('equal') # 保证长宽相等plt.pie(s, explode = [0.1,0,0,0], labels = s.index, colors=['r', 'g', 'b', 'c'], autopct='%.2f%%', pctdistance=0.6, labeldistance = 1.2, shadow = True, startangle=0, radius=1.5, frame=False)plt.show() 直方图plt.hist(x, bins=10, range=None, normed=False, weights=None, cumulative=False, bottom=None,histtype=’bar’, align=’mid’, orientation=’vertical’,rwidth=None, log=False, color=None, label=None,stacked=False, hold=None, data=None, **kwargs) 参数 bin：箱子的宽度 normed 标准化 histtype 风格，bar，barstacked，step，stepfilled orientation 水平还是垂直{‘horizontal’, ‘vertical’} align : {‘left’, ‘mid’, ‘right’}, optional(对齐方式) stacked：是否堆叠 1234567891011# 直方图s = pd.Series(np.random.randn(1000))s.hist(bins = 20, histtype = 'bar', align = 'mid', orientation = 'vertical', alpha=0.5, normed =True)# 密度图s.plot(kind='kde',style='k--')plt.show() 123D:\Program Files (x86)\Anaconda3\lib\site-packages\pandas\plotting\_core.py:2477: MatplotlibDeprecationWarning: The &apos;normed&apos; kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use &apos;density&apos; instead. ax.hist(values, bins=bins, **kwds) 123456789101112131415# 堆叠直方图plt.figure(num=1)df = pd.DataFrame(&#123;'a': np.random.randn(1000) + 1, 'b': np.random.randn(1000), 'c': np.random.randn(1000) - 1, 'd': np.random.randn(1000)-2&#125;, columns=['a', 'b', 'c','d'])df.plot.hist(stacked=True, bins=20, colormap='Greens_r', alpha=0.5, grid=True)# 使用DataFrame.plot.hist()和Series.plot.hist()方法绘制df.hist(bins=50)# 生成多个直方图 12345array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F34C1B710&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F34C4D2E8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F34C80898&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F34CB2E48&gt;]], dtype=object) 1&lt;Figure size 432x288 with 0 Axes&gt; 散点图plt.scatter(x, y, s=20, c=None, marker=’o’, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None,verts=None, edgecolors=None, hold=None, data=None, **kwargs) 参数含义： s：散点的大小 c：散点的颜色 vmin,vmax：亮度设置，标量 cmap：colormap 1234567891011plt.figure(figsize=(8,6))x = np.random.randn(1000)y = np.random.randn(1000)plt.scatter(x,y,marker='.', s = np.random.randn(1000)*100, cmap = 'Reds_r', c = y, alpha = 0.8,)plt.grid() 12D:\Program Files (x86)\Anaconda3\lib\site-packages\matplotlib\collections.py:857: RuntimeWarning: invalid value encountered in sqrt scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor 123456789101112# pd.scatter_matrix()散点矩阵# pd.scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, # grid=False, diagonal='hist', marker='.', density_kwds=None, hist_kwds=None, range_padding=0.05, **kwds)# diagonal：(&#123;‘hist’, ‘kde’&#125;)，必须且只能在&#123;‘hist’, ‘kde’&#125;中选择1个 → 每个指标的频率图# range_padding：(float, 可选)，图像在x轴、y轴原点附近的留白(padding)，该值越大，留白距离越大，图像远离坐标原点df = pd.DataFrame(np.random.randn(100,4),columns = ['a','b','c','d'])pd.plotting.scatter_matrix(df,figsize=(10,6), marker = 'o', diagonal='kde', alpha = 0.5, range_padding=0.5) 1234567891011121314151617array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F39EB5828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F222278&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F37E518&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F391F98&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F3AF128&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F3C5278&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F3DC3C8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F3F3550&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F3F3588&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F41FC18&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F43C208&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F4507B8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F466D68&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F483358&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F498908&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3F4AFEB8&gt;]], dtype=object) 箱型图箱型图：又称为盒须图、盒式图、盒状图或箱线图，是一种用作显示一组数据分散情况资料的统计图 包含一组数据的：最大值、最小值、中位数、上四分位数（Q1）、下四分位数（Q3）、异常值 ① 中位数 → 一组数据平均分成两份，中间的数 ② 下四分位数Q1 → 是将序列平均分成四份，计算(n+1)/4与(n-1)/4两种，一般使用(n+1)/4 ③ 上四分位数Q3 → 是将序列平均分成四份，计算(1+n)/4*3=6.75 ④ 内限 → T形的盒须就是内限，最大值区间Q3+1.5IQR,最小值区间Q1-1.5IQR （IQR=Q3-Q1） ⑤ 外限 → T形的盒须就是内限，最大值区间Q3+3IQR,最小值区间Q1-3IQR （IQR=Q3-Q1） ⑥ 异常值 → 内限之外 - 中度异常，外限之外 - 极度异常 plt.plot.box(),plt.boxplot() 123456789101112131415# plt.plot.box()绘制fig,axes = plt.subplots(2,1,figsize=(10,6))df = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])color = dict(boxes='DarkGreen', whiskers='DarkOrange', medians='DarkBlue', caps='Gray')# 箱型图着色# boxes → 箱线# whiskers → 分位数与error bar横线之间竖线的颜色# medians → 中位数线颜色# caps → error bar横线颜色df.plot.box(ylim=[0,1.2], grid = True, color = color, ax = axes[0]) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x24f394cc9b0&gt; 1234567891011121314151617181920212223242526272829303132333435df = pd.DataFrame(np.random.rand(10, 5), columns=['A', 'B', 'C', 'D', 'E'])plt.figure(figsize=(10,4))# 创建图表、数据f = df.boxplot(sym = 'o', # 异常点形状，参考marker vert = True, # 是否垂直 whis = 1.5, # IQR，默认1.5，也可以设置区间比如[5,95]，代表强制上下边缘为数据95%和5%位置 patch_artist = True, # 上下四分位框内是否填充，True为填充 meanline = False,showmeans=True, # 是否有均值线及其形状 showbox = True, # 是否显示箱线 showcaps = True, # 是否显示边缘线 showfliers = True, # 是否显示异常值 notch = False, # 中间箱体是否缺口 return_type='dict' # 返回类型为字典 ) plt.title('boxplot')for box in f['boxes']: box.set( color='b', linewidth=1) # 箱体边框颜色 box.set( facecolor = 'b' ,alpha=0.5) # 箱体内部填充颜色for whisker in f['whiskers']: whisker.set(color='k', linewidth=0.5,linestyle='-')for cap in f['caps']: cap.set(color='gray', linewidth=2)for median in f['medians']: median.set(color='DarkBlue', linewidth=2)for flier in f['fliers']: flier.set(marker='o', color='y', alpha=0.5)# boxes, 箱线# medians, 中位值的横线,# whiskers, 从box到error bar之间的竖线.# fliers, 异常值# caps, error bar横线# means, 均值的横线, 1234567891011# plt.boxplot()绘制# 分组汇总df = pd.DataFrame(np.random.rand(10,2), columns=['Col1', 'Col2'] )df['X'] = pd.Series(['A','A','A','A','A','B','B','B','B','B'])df['Y'] = pd.Series(['A','B','A','B','A','B','A','B','A','B'])df.boxplot(by = 'X')df.boxplot(column=['Col1','Col2'], by=['X','Y'])# columns：按照数据的列分子图# by：按照列分组做箱型图 123array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F3519AD68&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000024F34B67C88&gt;], dtype=object)]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matplotlib的Seaborn风格可视化]]></title>
    <url>%2F2019%2F10%2F04%2FMatplotlib%E7%9A%84Seaborn%E9%A3%8E%E6%A0%BC%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Seaborn风格可视化什么是seaborn​ Seaborn是基于matplotlib的图形可视化python包。它提供了一种高度交互式界面，便于用户能够做出各种有吸引力的统计图表。Seaborn是在matplotlib的基础上进行了更高级的API封装，从而使得作图更加容易，在大多数情况下使用seaborn能做出很具有吸引力的图，而使用matplotlib就能制作具有更多特色的图。应该把Seaborn视为matplotlib的补充，而不是替代物。同时它能高度兼容numpy与pandas数据结构以及scipy与statsmodels等统计模式。 seaborn APISeaborn 要求原始数据的输入类型为 pandas 的 Dataframe 或 Numpy 数组，画图函数有以下几种形式: sns.图名(x=’X轴 列名’, y=’Y轴 列名’, data=原始数据df对象) sns.图名(x=’X轴 列名’, y=’Y轴 列名’, hue=’分组绘图参数’, data=原始数据df对象) sns.图名(x=np.array, y=np.array[, …]) 123456import numpy as npimport pandas as pdimport scipy as statsimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline 基本绘图设置1234567# 创建正弦函数def sinplot(flip=1): x = np.linspace(0, 14, 100) for i in range(1,7): plt.plot(x, np.sin(x+i*.5)*(7-i)*flip)sinplot() 简单切换成Seaborn风格1234# 切换Seaborn风格sns.set()fig = plt.figure(figsize=(8,6))sinplot() 1234567891011# 切换seaborn图标风格fig = plt.figure(figsize=(10,6), facecolor='white')ax1 = fig.add_subplot(211)sns.set_style('whitegrid')data = np.random.normal(size=(20,6))+np.arange(6)/2sns.boxplot(data=data)plt.title('style-whitegrid')ax2 = fig.add_subplot(212)sns.set_style('dark')sinplot() 设置图标坐标轴1234567891011121314151617181920212223242526#despine()# seaborn.despine(fig=None, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)# 设置风格sns.set_style("ticks")# 创建图表fig = plt.figure(figsize=(6,9))plt.subplots_adjust(hspace=0.3)ax1 = fig.add_subplot(3,1,1) sinplot()# 删除了上、右坐标轴sns.despine()ax2 = fig.add_subplot(3,1,2)sns.violinplot(data=data)# offset：与坐标轴之间的偏移# trim：为True时，将坐标轴限制在数据最大最小值#sns.despine(offset=10, trim=True)ax3 = fig.add_subplot(3,1,3)# top, right, left, bottom：布尔型，为True时不显示#sns.despine(left=True, right = False)sns.boxplot(data=data, palette="deep") 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b20e4f7a58&gt; 设置局部图标风格123456789with sns.axes_style("darkgrid"): plt.subplot(211) sinplot()# 设置局部图表风格，用with做代码块区分sns.set_style("whitegrid")plt.subplot(212)sinplot()# 外部表格风格 设置显示比例123456789#set_context()# 选择包括：'paper', 'notebook', 'talk', 'poster'## 与上面的cell比较你就会发现不同 sns.set_style("whitegrid")sns.set_context("poster")plt.subplot(212)sinplot() 调色板123456789# color_palette()# 默认6种颜色：deep, muted, pastel, bright, dark, colorblind# seaborn.color_palette(palette=None, n_colors=None, desat=None)current_palette = sns.color_palette()print(type(current_palette))# sns.palplot(current_palette[2:4])sns.palplot(current_palette) 1&lt;class &apos;seaborn.palettes._ColorPalette&apos;&gt; 颜色风格1234567891011121314# 颜色风格内容：Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, # BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, # Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples,# Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, # Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, # autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, # cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, # gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, # hsv_r, icefire, icefire_r, inferno, inferno_r, jet, jet_r, magma, magma_r, mako, mako_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, # pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, rocket, rocket_r, seismic, seismic_r, spectral, spectral_r, spring, # spring_r, summer, summer_r, terrain, terrain_r, viridis, viridis_r, vlag, vlag_r, winter, winter_rsns.palplot(sns.color_palette('Accent',12))sns.palplot(sns.color_palette('Accent_r',8)) 设置饱和度和亮度1234sns.palplot(sns.hls_palette(4,l=.3,s=.8))# l-&gt;亮度# s-&gt;饱和度 设置颜色线性变化1234567891011#设置颜色线性变化sns.palplot(sns.cubehelix_palette(16, gamma=2))sns.palplot(sns.cubehelix_palette(16, start=.5, rot=.75))sns.palplot(sns.cubehelix_palette(16,start=0.5, rot=0, dark=0.95, reverse=True))# n_colors → 颜色个数# start → 值区间在0-3，开始颜色# rot → 颜色旋转角度# gamma → 颜色伽马值，越大颜色越暗# dark，light → 值区间0-1，颜色深浅# reverse → 布尔值，默认为False，由浅到深 创建分散颜色123456plt.figure(figsize = (8,6))x = np.arange(25).reshape(5, 5)# 创建分散颜色cmap = sns.diverging_palette(200, 20, sep=20, as_cmap=True)sns.heatmap(x, cmap=cmap) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21a370cf8&gt; 123456789sns.set_style('whitegrid')fig=plt.figure(figsize=(12,8))with sns.color_palette('PuBuGn_d'): plt.subplot(211) sinplot() sns.set_palette('husl')plt.subplot(212)sinplot() 123456sns.set_style('darkgrid')sns.set_context('paper')import warningswarnings.filterwarnings('ignore')#不再发出警告 分布数据可视化直方图12345678910111213141516#直方图#设计随即种子rs = np.random.RandomState(10)s = pd.Series(rs.randn(100)*100)sns.distplot(s, bins=10, hist=True, kde=False, norm_hist=False, rug=True,vertical=False,color='y', label='distplot', axlabel='x')plt.legend()# bins → 箱数# hist、ked → 是否显示箱/密度曲线# norm_hist → 直方图是否按照密度来显示# rug → 是否显示数据分布情况# vertical → 是否水平显示# color → 设置颜色# label → 图例# axlabel → x轴标注 1&lt;matplotlib.legend.Legend at 0x1b20e65e4e0&gt; 123456sns.distplot(s, rug=True, rug_kws=&#123;'color':'g'&#125;, kde_kws=&#123;"color": "k", "lw": 1, "label": "KDE",'linestyle':'--'&#125;, # 设置密度曲线颜色，线宽，标注、线形 hist_kws=&#123;"histtype": "step", "linewidth": 1,"alpha": 1, "color": "g"&#125;) # 设置箱子的风格、线宽、透明度、颜色 # 风格包括：'bar', 'barstacked', 'step', 'stepfilled' 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21bc8e828&gt; 密度图123456789101112131415161718# 密度图 - kdeplot()# 单个样本数据密度分布图sns.kdeplot(s, shade = False, # 是否填充 color = 'b', # 设置颜色 vertical = False # 设置是否水平 )sns.kdeplot(s,bw=5, label="bw: 0.2", linestyle = '-',linewidth = 1.2,alpha = 0.5)sns.kdeplot(s,bw=20, label="bw: 2", linestyle = '-',linewidth = 1.2,alpha = 0.5)# bw → 控制拟合的程度，类似直方图的箱数sns.rugplot(s,height = 0.1,color = 'k',alpha = 0.5)# 数据频率分布图 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21babf470&gt; 123456789101112131415161718# 密度图 - kdeplot()# 两个样本数据密度分布图rs = np.random.RandomState(2) # 设定随机数种子df = pd.DataFrame(rs.randn(100,2), columns = ['A','B'])sns.kdeplot(df['A'],df['B'], cbar = True, # 是否显示颜色图例 shade = True, # 是否填充 cmap = 'Reds', # 设置调色盘 shade_lowest=False, # 最外围颜色是否显示 n_levels = 10 # 曲线个数（如果非常多，则会越平滑） )# 两个维度数据生成曲线密度图，以颜色作为密度衰减显示sns.rugplot(df['A'], color="g", axis='x',alpha = 0.5)sns.rugplot(df['B'], color="r", axis='y',alpha = 0.5)# 注意设置x，y轴 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21bb63470&gt; 1234567891011121314151617# 密度图 - kdeplot()# 两个样本数据密度分布图# 多个密度图rs1 = np.random.RandomState(2) rs2 = np.random.RandomState(5) df1 = pd.DataFrame(rs1.randn(100,2)+2,columns = ['A','B'])df2 = pd.DataFrame(rs2.randn(100,2)-2,columns = ['A','B'])# 创建数据sns.kdeplot(df1['A'],df1['B'],cmap = 'Greens', shade = True,shade_lowest=False)sns.kdeplot(df2['A'],df2['B'],cmap = 'Blues', shade = True,shade_lowest=False)# 创建图表#sns.rugplot(df2['A']+df1['A'], color="g", axis='x',alpha = 0.5)#sns.rugplot(df2['B']+df1['B'], color="r", axis='y',alpha = 0.5) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21be56278&gt; 综合散点图1234567891011121314151617# 综合散点图 - jointplot()# 散点图 + 分布图rs = np.random.RandomState(2) df = pd.DataFrame(rs.randn(200,2),columns = ['A','B'])sns.jointplot(x=df['A'], y=df['B'], # 设置xy轴，显示columns名称 data=df, # 设置数据 color = 'k', # 设置颜色 s = 50, edgecolor="w",linewidth=1, # 设置散点大小、边缘线颜色及宽度(只针对scatter） kind = 'scatter', # 设置类型：“scatter”、“reg”、“resid”、“kde”、“hex” space = 0.2, # 设置散点图和布局图的间距 size = 8, # 图表大小（自动调整为正方形） ratio = 5, # 散点图与布局图高度比，整型 marginal_kws=dict(bins=15, rug=True) # 设置柱状图箱数，是否设置rug ) 1&lt;seaborn.axisgrid.JointGrid at 0x1b21bee2be0&gt; 12345678910# 综合散点图 - jointplot()# 散点图 + 分布图# 六边形图df = pd.DataFrame(rs.randn(500,2),columns = ['A','B'])with sns.axes_style("white"): sns.jointplot(x=df['A'], y=df['B'],data = df, kind="hex", color="g", marginal_kws=dict(bins=20)) 123456789101112131415# 综合散点图 - jointplot()# 散点图 + 分布图# 密度图rs = np.random.RandomState(15)df = pd.DataFrame(rs.randn(300,2),columns = ['A','B'])# 创建数据g = sns.jointplot(x=df['A'], y=df['B'],data = df, kind="kde", color="k", shade_lowest=False)# 创建密度图g.plot_joint(plt.scatter,c="w", s=30, linewidth=1, marker="*")# 添加散点图 1&lt;seaborn.axisgrid.JointGrid at 0x1b21c4325f8&gt; 1234567891011121314151617181920212223242526# 综合散点图 - JointGrid()# 可拆分绘制的散点图# plot_joint() + ax_marg_x.hist() + ax_marg_y.hist()sns.set_style("white")# 设置风格tips = sns.load_dataset("tips")print(tips.head())# 导入数据g = sns.JointGrid(x="total_bill", y="tip", data=tips)# 创建一个绘图表格区域，设置好x、y对应数据g.plot_joint(plt.scatter, color ='m', edgecolor = 'white') # 设置框内图表，scatterg.ax_marg_x.hist(tips["total_bill"], color="b", alpha=.6, bins=np.arange(0, 60, 3)) # 设置x轴直方图，注意bins是数组g.ax_marg_y.hist(tips["tip"], color="r", alpha=.6, orientation="horizontal", bins=np.arange(0, 12, 1)) # 设置x轴直方图，注意需要orientation参数from scipy import statsg.annotate(stats.pearsonr) # 设置标注，可以为pearsonr，spearmanrplt.grid(linestyle = '--') 123456 total_bill tip sex smoker day time size0 16.99 1.01 Female No Sun Dinner 21 10.34 1.66 Male No Sun Dinner 32 21.01 3.50 Male No Sun Dinner 33 23.68 3.31 Male No Sun Dinner 24 24.59 3.61 Female No Sun Dinner 4 1234567891011# 综合散点图 - JointGrid()# 可拆分绘制的散点图# plot_joint() + plot_marginals()g = sns.JointGrid(x="total_bill", y="tip", data=tips)# 创建一个绘图表格区域，设置好x、y对应数据g = g.plot_joint(plt.scatter,color="g", s=40, edgecolor="white") # 绘制散点图plt.grid(linestyle = '--')g.plot_marginals(sns.distplot, kde=True, color="g") # 绘制x，y轴直方图 1&lt;seaborn.axisgrid.JointGrid at 0x1b21c630da0&gt; 123456789101112# 综合散点图 - JointGrid()# 可拆分绘制的散点图# plot_joint() + plot_marginals()# kde - 密度图g = sns.JointGrid(x="total_bill", y="tip", data=tips)# 创建一个绘图表格区域，设置好x、y对应数据g = g.plot_joint(sns.kdeplot,cmap = 'Reds_r') # 绘制密度图plt.grid(linestyle = '--')g.plot_marginals(sns.kdeplot, shade = True, color="r") # 绘制x，y轴密度图 1&lt;seaborn.axisgrid.JointGrid at 0x1b21d7aef60&gt; 矩阵散点图1234567891011121314151617# 矩阵散点图 - pairplot()sns.set_style("white")# 设置风格iris = sns.load_dataset("iris")print(iris.head())# 读取数据sns.pairplot(iris, kind = 'scatter', # 散点图/回归分布图 &#123;‘scatter’, ‘reg’&#125; diag_kind="hist", # 直方图/密度图 &#123;‘hist’, ‘kde’&#125; hue="species", # 按照某一字段进行分类 palette="husl", # 设置调色板 markers=["o", "s", "D"], # 设置不同系列的点样式（这里根据参考分类个数） size = 2, # 图表大小 ) 123456 sepal_length sepal_width petal_length petal_width species0 5.1 3.5 1.4 0.2 setosa1 4.9 3.0 1.4 0.2 setosa2 4.7 3.2 1.3 0.2 setosa3 4.6 3.1 1.5 0.2 setosa4 5.0 3.6 1.4 0.2 setosa 1&lt;seaborn.axisgrid.PairGrid at 0x1b21d8a44e0&gt; 123456# 矩阵散点图 - pairplot()# 只提取局部变量进行对比sns.pairplot(iris,vars=["sepal_width", "sepal_length"], kind = 'reg', diag_kind="kde", hue="species", palette="husl") 1&lt;seaborn.axisgrid.PairGrid at 0x1b21e003c18&gt; 123456789# 矩阵散点图 - pairplot()# 其他参数设置sns.pairplot(iris, diag_kind="kde", markers="+", plot_kws=dict(s=50, edgecolor="b", linewidth=1), # 设置点样式 diag_kws=dict(shade=True) # 设置密度图样式 ) 1&lt;seaborn.axisgrid.PairGrid at 0x1b21c37be48&gt; 123456789101112131415161718192021# 矩阵散点图 - PairGrid()# 可拆分绘制的散点图# map_diag() + map_offdiag()g = sns.PairGrid(iris,hue="species",palette = 'hls', vars = ['sepal_length','sepal_width','petal_length','petal_width'], # 可筛选 )# 创建一个绘图表格区域，设置好x、y对应数据，按照species分类g.map_diag(plt.hist, histtype = 'barstacked', # 可选：'bar', 'barstacked', 'step', 'stepfilled' linewidth = 1, edgecolor = 'w') # 对角线图表，plt.hist/sns.kdeplotg.map_offdiag(plt.scatter, edgecolor="w", s=40,linewidth = 1, # 设置点颜色、大小、描边宽度 ) # 其他图表，plt.scatter/plt.bar...g.add_legend()# 添加图例 1&lt;seaborn.axisgrid.PairGrid at 0x1b218fe3f98&gt; 12345678# 矩阵散点图 - PairGrid()# 可拆分绘制的散点图# map_diag() + map_lower() + map_upper()g = sns.PairGrid(iris)g.map_diag(sns.kdeplot, lw=3) # 设置对角线图表g.map_upper(plt.scatter, color = 'r') # 设置对角线上端图表g.map_lower(sns.kdeplot, cmap="Blues_d") # 设置对角线下端图表 1&lt;seaborn.axisgrid.PairGrid at 0x1b21ee966a0&gt; 分类数据可视化分类散点图1234567891011121314# stripplot()# 按照不同类别对样本数据进行分布散点图绘制tips = sns.load_dataset("tips")print(tips.head())sns.stripplot(x="day", # x → 设置分组统计字段 y="total_bill", # y → 数据分布统计字段 # 这里xy数据对调，将会使得散点图横向分布 data=tips, # data → 对应数据 jitter = True, # jitter → 当点数据重合较多时，用该参数做一些调整，也可以设置间距如：jitter = 0.1 size = 5, edgecolor = 'w',linewidth=1,marker = 'o' # 设置点的大小、描边颜色或宽度、点样式 ) 123456 total_bill tip sex smoker day time size0 16.99 1.01 Female No Sun Dinner 21 10.34 1.66 Male No Sun Dinner 32 21.01 3.50 Male No Sun Dinner 33 23.68 3.31 Male No Sun Dinner 24 24.59 3.61 Female No Sun Dinner 4 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21f971320&gt; 12345# stripplot()# 通过hue参数再分类sns.stripplot(x="sex", y="total_bill", hue="day", data=tips, jitter=True) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21f9b2b00&gt; 12345678# stripplot()# 设置调色盘sns.stripplot(x="sex", y="total_bill", hue="day", data=tips, jitter=True, palette="Set2", # 设置调色盘 dodge=True, # 是否拆分 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21fc11198&gt; 123456789# stripplot()# 筛选分类类别print(tips['day'].value_counts())# 查看day字段的唯一值sns.stripplot(x="day", y="total_bill", data=tips,jitter = True, order = ['Sat','Sun'])# order → 筛选类别 12345Sat 87Sun 76Thur 62Fri 19Name: day, dtype: int64 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21fc8c748&gt; 分簇散点图1234567# swarmplot()# 分簇散点图sns.swarmplot(x="total_bill", y="day", data=tips, size = 5, edgecolor = 'w',linewidth=1,marker = 'o', palette = 'Reds')# 用法和stripplot类似 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21fcdef28&gt; 箱型图123456789101112131415# boxplot()sns.boxplot(x="day", y="total_bill", data=tips, linewidth = 2, # 线宽 width = 0.8, # 箱之间的间隔比例 fliersize = 3, # 异常点大小 palette = 'hls', # 设置调色板 whis = 1.5, # 设置IQR notch = True, # 设置是否以中值做凹槽 order = ['Thur','Fri','Sat','Sun'], # 筛选类别 )# 绘制箱型图#sns.swarmplot(x="day", y="total_bill", data=tips,color ='k',size = 3,alpha = 0.8)# 可以添加散点图 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21fd32710&gt; 123# 通过hue参数再分类sns.boxplot(x="day", y="total_bill", data=tips, hue = 'smoker', palette = 'Reds') 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21fdce5c0&gt; 小提琴图12345678910111213# violinplot()sns.violinplot(x="day", y="total_bill", data=tips, linewidth = 2, # 线宽 width = 0.8, # 箱之间的间隔比例 palette = 'hls', # 设置调色板 order = ['Thur','Fri','Sat','Sun'], # 筛选类别 scale = 'area', # 测度小提琴图的宽度：area-面积相同，count-按照样本数量决定宽度，width-宽度一样 gridsize = 50, # 设置小提琴图边线的平滑度，越高越平滑 inner = 'box', # 设置内部显示类型 → “box”, “quartile”, “point”, “stick”, None #bw = 0.8 # 控制拟合程度，一般可以不设置 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21feb0d68&gt; 123456# 通过hue参数再分类sns.violinplot(x="day", y="total_bill", data=tips, hue = 'smoker', palette="muted", split=True, # 设置是否拆分小提琴图 inner="quartile") 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21ff37940&gt; 12345# 结合散点图sns.violinplot(x="day", y="total_bill", data=tips, palette = 'hls', inner = None)sns.swarmplot(x="day", y="total_bill", data=tips, color="w", alpha=.5)# 插入散点图 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21fff0e80&gt; LV图123456789101112# lvplot()sns.lvplot(x="day", y="total_bill", data=tips, palette="mako", #hue = 'smoker', width = 0.8, # 箱之间间隔比例 linewidth = 12, scale = 'area', # 设置框的大小 → “linear”、“exonential”、“area” k_depth = 'proportion', # 设置框的数量 → “proportion”、“tukey”、“trustworthy” )# 绘制LV图sns.swarmplot(x="day", y="total_bill", data=tips,color ='k',size = 3,alpha = 0.8)# 可以添加散点图 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b22101c400&gt; 分类统计图1234567891011121314151617181920# barplot()# 柱状图 - 置信区间估计# 置信区间：样本均值 + 抽样误差titanic = sns.load_dataset("titanic")#print(titanic.head())#print('-----')# 加载数据sns.barplot(x="sex", y="survived", hue="class", data=titanic, palette = 'hls', order = ['male','female'], # 筛选类别 capsize = 0.05, # 误差线横向延伸宽度 saturation=.8, # 颜色饱和度 errcolor = 'gray',errwidth = 2, # 误差线颜色，宽度 ci = 'sd' # 置信区间误差 → 0-100内值、'sd'、None )#print(titanic.groupby(['sex','class']).mean()['survived'])#print(titanic.groupby(['sex','class']).std()['survived'])# 计算数据 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b2210a1048&gt; 1234567# barplot()# 柱状图 - 置信区间估计sns.barplot(x="day", y="total_bill", hue="sex", data=tips, palette = 'Blues',edgecolor = 'w')tips.groupby(['day','sex']).mean()# 计算数据 .dataframe tbody tr th:only-of-type { vertical-align: middle; } 1234567.dataframe tbody tr th &#123; vertical-align: top;&#125;.dataframe thead th &#123; text-align: right;&#125; total_bill tip size day sex Thur Male 18.714667 2.980333 2.433333 Female 16.715312 2.575625 2.468750 Fri Male 19.857000 2.693000 2.100000 Female 14.145556 2.781111 2.111111 Sat Male 20.802542 3.083898 2.644068 Female 19.680357 2.801786 2.250000 Sun Male 21.887241 3.220345 2.810345 Female 19.872222 3.367222 2.944444 12345678910111213141516171819202122# 1、barplot()# 柱状图 - 置信区间估计crashes = sns.load_dataset("car_crashes").sort_values("total", ascending=False)print(crashes.head())# 加载数据f, ax = plt.subplots(figsize=(6, 15))# 创建图表sns.set_color_codes("pastel")sns.barplot(x="total", y="abbrev", data=crashes, label="Total", color="b",edgecolor = 'w')# 设置第一个柱状图sns.set_color_codes("muted")sns.barplot(x="alcohol", y="abbrev", data=crashes, label="Alcohol-involved", color="b",edgecolor = 'w')# 设置第二个柱状图ax.legend(ncol=2, loc="lower right")sns.despine(left=True, bottom=True) 12345678910111213 total speeding alcohol not_distracted no_previous ins_premium \40 23.9 9.082 9.799 22.944 19.359 858.97 34 23.9 5.497 10.038 23.661 20.554 688.75 48 23.8 8.092 6.664 23.086 20.706 992.61 3 22.4 4.032 5.824 21.056 21.280 827.34 17 21.4 4.066 4.922 16.692 16.264 872.51 ins_losses abbrev 40 116.29 SC 34 109.72 ND 48 152.56 WV 3 142.39 AR 17 137.13 KY 1234567# countplot()# 计数柱状图sns.countplot(x="class", hue="who", data=titanic,palette = 'magma')#sns.countplot(y="class", hue="who", data=titanic,palette = 'magma') # x/y → 以x或者y轴绘图（横向，竖向）# 用法和barplot相似 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b22117aac8&gt; 123456789101112# pointplot()# 折线图 - 置信区间估计sns.pointplot(x="time", y="total_bill", hue = 'smoker',data=tips, palette = 'hls', dodge = True, # 设置点是否分开 join = True, # 是否连线 markers=["o", "x"], linestyles=["-", "--"], # 设置点样式、线型 )tips.groupby(['time','smoker']).mean()['total_bill']# 计算数据# # 用法和barplot相似 123456time smokerLunch Yes 17.399130 No 17.050889Dinner Yes 21.859429 No 20.095660Name: total_bill, dtype: float64 线性数据可视化基本使用1234567891011# 基本用法tips = sns.load_dataset("tips")print(tips.head())# 加载数据sns.lmplot(x="total_bill", y="tip", hue = 'smoker',data=tips,palette="Set1", ci = 70, # 误差值 size = 5, # 图表大小 markers = ['+','o'], # 点样式 ) 123456 total_bill tip sex smoker day time size0 16.99 1.01 Female No Sun Dinner 21 10.34 1.66 Male No Sun Dinner 32 21.01 3.50 Male No Sun Dinner 33 23.68 3.31 Male No Sun Dinner 24 24.59 3.61 Female No Sun Dinner 4 1&lt;seaborn.axisgrid.FacetGrid at 0x1b21c57d7b8&gt; 多表格1sns.lmplot(x="total_bill", y="tip", col="smoker", data=tips) 1&lt;seaborn.axisgrid.FacetGrid at 0x1b2215774e0&gt; 1234567# 多图表1sns.lmplot(x="size", y="total_bill", hue="day", col="day",data=tips, aspect=0.6, # 长宽比 x_jitter=.30, # 给x或者y轴随机增加噪音点 col_wrap=4, # 每行的列数 ) 1&lt;seaborn.axisgrid.FacetGrid at 0x1b2216276a0&gt; 12345# 多图表2sns.lmplot(x="total_bill", y="tip", row="sex", col="time",data=tips, size=4)# 行为sex字段，列为time字段# x轴total_bill, y轴tip 1&lt;seaborn.axisgrid.FacetGrid at 0x1b22160a400&gt; 非线性回归1234# 非线性回归sns.lmplot(x="total_bill", y="tip",data=tips, order = 2) 1&lt;seaborn.axisgrid.FacetGrid at 0x1b2214d7b00&gt; 其他图表可视化时间线图123456789101112131415# tsplot()x = np.linspace(0, 15, 31)data = np.sin(x) + np.random.rand(10, 31) + np.random.randn(10, 1)#print(data.shape)#print(pd.DataFrame(data).head())# 创建数据sns.tsplot(data=data, err_style="ci_band", # 误差数据风格，可选：ci_band, ci_bars, boot_traces, boot_kde, unit_traces, unit_points interpolate=True, # 是否连线 ci = [40,70,90], # 设置误差区间 color = 'r' # 设置颜色 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b21668c860&gt; 123sns.tsplot(data=data, err_style="boot_traces", n_boot=300 # 迭代次数 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b216533048&gt; 12345678910111213gammas = sns.load_dataset("gammas")print(gammas.head())print('数据量为：%i条' % len(gammas))print('timepoint为0.0时的数据量为：%i条' % len(gammas[gammas['timepoint'] == 0]))print('timepoint共有%i个唯一值' % len(gammas['timepoint'].value_counts()))#print(gammas['timepoint'].value_counts()) # 查看唯一值具体信息# 导入数据sns.tsplot(time="timepoint", # 时间数据，x轴 value="BOLD signal", # y轴value unit="subject", # condition="ROI", # 分类 data=gammas) 123456789 timepoint ROI subject BOLD signal0 0.0 IPS 0 0.5134331 0.0 IPS 1 -0.4143682 0.0 IPS 2 0.2146953 0.0 IPS 3 0.8148094 0.0 IPS 4 -0.894992数据量为：6000条timepoint为0.0时的数据量为：60条timepoint共有100个唯一值 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b221f95a58&gt; 热图123456789# 热图 - heatmap()# 简单示例df = pd.DataFrame(np.random.rand(10,15))# 创建数据 - 10*12图表sns.heatmap(df, # 加载数据 vmin=0, vmax=1 # 设置图例最大最小值 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b221faac88&gt; 123456789101112131415161718# heatmap()# 参数设置flights = sns.load_dataset("flights")flights = flights.pivot("month", "year", "passengers") #print(flights.head())# 加载数据 sns.heatmap(flights, annot = True, # 是否显示数值 fmt = 'd', # 格式化字符串 linewidths = 0.2, # 格子边线宽度 #center = 100, # 调色盘的色彩中心值，若没有指定，则以cmap为主 #cmap = 'Reds', # 设置调色盘 cbar = True, # 是否显示图例色带 #cbar_kws=&#123;"orientation": "horizontal"&#125;, # 是否横向显示图例色带 #square = True, # 是否正方形显示图表 ) 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b223040588&gt; 123456789101112131415161718192021# heatmap()# 绘制半边热图sns.set(style="white")# 设置风格rs = np.random.RandomState(33)d = pd.DataFrame(rs.normal(size=(100, 26)))corr = d.corr() # 求解相关性矩阵表格# 创建数据mask = np.zeros_like(corr, dtype=np.bool)mask[np.triu_indices_from(mask)] = True# 设置一个“上三角形”蒙版cmap = sns.diverging_palette(220, 10, as_cmap=True)# 设置调色盘sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=0.2)# 生成半边热图 1&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b2231f3128&gt; 图标矩阵12345678910111213141516attend = sns.load_dataset("attention")print(attend.head())# 加载数据g = sns.FacetGrid(attend, col="subject", col_wrap=5, # 设置每行的图表数量 size=1.5)g.map(plt.plot, "solutions", "score", marker="o",color = 'gray',linewidth = 2)# 绘制图表矩阵g.set(xlim = (0,4), ylim = (0,10), xticks = [0,1,2,3,4], yticks = [0,2,4,6,8,10] )# 设置x，y轴刻度 123456 Unnamed: 0 subject attention solutions score0 0 1 divided 1 2.01 1 2 divided 1 3.02 2 3 divided 1 3.03 3 4 divided 1 5.04 4 5 divided 1 4.0 1&lt;seaborn.axisgrid.FacetGrid at 0x1b22328cb00&gt;]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>可视化</tag>
        <tag>Seaborn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA主成分分析]]></title>
    <url>%2F2019%2F09%2F26%2FPCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[S_B=(u_1-u_2)(u_1-u_2)^T S_w^{-1}(u_1-u_2)(u_1-u_2)^Tw=\lambda W因为$(u_1-u_2)^Tw$是一个标量，所以可以看出 W\propto S_W^{-1}(u_1-u_2)需要注意的是$S_W$在大多数情况下是不可逆的，所以为了解决这个问题，通常有两种方法 令$S_W=S_W+\gamma I$，其中$I$是一个特别小的数，这样$S_W$一定可逆 先使用PCA对数据局进行降维，使得降维后的数据的$S_W$一定可逆 S_w=\sum_{i=1}^C S_{wi} S_{wi}=\sum_{x\in{y_i}}(x-\mu_i)(x-\mu_i)^T S_B=\sum_{i=1}^C\frac{N_i}{N}(\mu_i-\mu)(\mu_i-\mu)^T其中$\mu$表示所有的特征值得均值 \mu=\frac{1}{N}\sum_{\forall x\in y_i}x而$\mu_i$求的是每一个分类的均值，在而分类中$S_B$表示的是类间的差值，但是在多分类中肯定无法这样计算，所以分类中计算的是每个分类中心点到所以分类中心点的方差。C表示的是分类数。 最后得到的结果和原来还是一样 S_w^{-1}S_Bw_i=\lambda w_i所以总结下来LDA的计算流程为 计算每个分类的特征中心值$\mu$ 计算每个分类的类内方差$S_W$ 计算每个分类的类间方差$S_B$ 计算评价函数$J(w)=\frac{wS_Bw^t}{wS_ww^t}$ 利用拉格朗日得到最后的结果 LDA用于降维，和PCA有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。 首先我们看看相同点： 1）两者均可以对数据进行降维。 2）两者在降维时均使用了矩阵特征分解的思想。 3）两者都假设数据符合高斯分布。 我们接着看看不同点： 1）LDA是有监督的降维方法，而PCA是无监督的降维方法 2）LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。，所以 3）LDA除了可以用于降维，还可以用于分类。 4）LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。 这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优。 感觉使用latex写公式真的爽的一笔啊 f(x)=\frac{1}{x}+3y+7z]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>特征降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘比赛技巧]]></title>
    <url>%2F2019%2F09%2F21%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AF%94%E8%B5%9B%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[特征工程 缺失值填充 特征为连续值，且为正态分布，使用均值填充，保持期望不变 特征值为连续值，且为长尾分布，使用中值填充，避免异常点影响 特征为离散值，使用众数填充 使用模型预测完善用户画像 特征转换 对长尾分布的特征，做对数变换 标准化、归一化 连续值特征离散化 基于LR、SVM、DNN等对特征的分布和尺度敏感的，归一化有助于模型收敛，基于树模型，具有伸缩不变形，不需要做特征变换 ID类特征处理 OneHot编码，例如性别，编码为0,1或者1,0 使用某种特征的统计量代替该特征 Word Embedding，将高纬稀疏特征映射到低纬稠密特征。 异常值剔除 模型选择1、对于高维稀疏特征(如ID特征 One hot编码后)，使用线性模型LR、FM(腾讯社交广告大赛) 2、对于低纬稠密特征，使用集成树模型XgBoost，GDBT，Random Forest(o2o优惠券核销预测) 3、对于图像语音类数据，使用DNN，如CNN，LSTM 数据挖掘比赛中集成树模型占优势的原因： 比赛数据特点 结构化标单数据 混合类型(类别型，连续型) 大量缺失值 含有离群点 长尾分布 树算法模型法特点 善于处理混合类型特征 善于处理缺失值 伸缩不变性 对离群点有鲁棒性 容易并行化、有高效开源工具 模型融合Average、Voting、Stacking，Blending Stacking工具mlxtend 调参经验和技巧 树模型调参经验 GridSearchCV]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>数据降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive的基本操作]]></title>
    <url>%2F2019%2F09%2F17%2Fhive%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[表的增删改查操作创建表使用if not exists 如果存在则跳过，comment为注释。123456789create table if not exists mydb.employees( name string comment 'Employee name', salary float comment 'Employee salary', subordinates array&lt;string&gt; comment 'Names of subordinates', deduction map&lt;string, float&gt;, address struct&lt;street:string, city:string, state:string, zip:int&gt; comment 'Home address')comment 'descriptions of table'location '/user/hive/warehouse/mydb.db/employees'; 描述表显示表的字段和结构，使用desc/describe 1234--显示表的字段和数据类型desc table_name;--显示对应字段的数据类型desc table_name.columns 管理表和外部表管理表是hive创建的表，由hive控制其生命周期，hive默认情况下会将数据存在在配置文件指定的目录当中，由hive.metastore.warehouse.dir指定。当使用hive删除表的时候，对应的数据也会被删除，即hdfs文件系统中的数据也会被删除。管理表的缺点在于无法共享数据，比如利用pig等工具创建的数据，hive对其没有权限。当使用hive查询这些数据的时候就可以使用一个外部表指向这份数据，而不需要对其的权限。外部表需要使用external修饰。 123456789101112create external table if not exists stocks( exchange string, symbol string, ymd string, price__open string, price__high string, price__low string, price__close float, volume int, price_adj_close float)row format delimited fields terminated by ',' '分隔符为,'location 'data/stocks'; 加上external字段值后，删除表并不会删除这份数据，不过描述标的元数据信息会被删除。元数据可以理解为对该表的描述信息，而不是表内数据。 需要注意的是如果语句省略了external关键字同事源表是外部表，那么新表也是外部表，如果源表是管理表，新表也是管理表。在加上external之后，无论源表是管理表还是外部表，新表都是外部表。 分区表在建表过程中，会根据分区字段创建对应目录，优点在于分层存储，可以加快查询速度，而缺点在于一些数据存在于文件目录下，但是hive只能从表中读取数据，因此会造成资源浪费。分区表创建： 12345678create table employees( name string, salary float, subordinates array&lt;string&gt;, deduction map&lt;string,float&gt;, address struct&lt;street:string, city:string, city:string, state:string&gt;)partitioned by (country string, state string); 在建表的时候hive在hdfs上的目录为…/employees/country/state 查看表中存在所有分区 1show partitions employees; 查询指定分区 1show partitions employees partition(country='CHINA') 删除表1drop table if exists table_name; 对于管理表，表的元数据和表内数据都会被删除。 修改表 表的重命名将表从ａ重命名为ｂ 1alter table a rename to b; 增加表分区1234567alter table add partiion--在一个查询语句中增加多个分区alter table table_name if not exists partition(...) location '/user/hive/warehouse/a'partition(...) location '/user/hive/warehouse/b'partition(...) location '/user/hive/warehouse/c' 修改列的信息将列名从a改到b，并且将其移到serverity字段后面。 1234alter table table_name change column a b type_name '修改列的数据类型'comment 'xxx'after serverity 增加新的列1234alter table table_name add column( app_name string comment 'Application name', session_id long comment 'the current session id';) 删除或替换列1234alter table table_name replace columns（ hour_mins_secs INT comment 'xxx' severity string comment 'xxx';) 将之前的列都删除，只留下replace的列 修改表的属性1alter table table_name set tblproperties('notes'='xxx'); 修改表的存储属性1alter table table_name partition(a=xxx,b=xxx,c=xxx) set fileformat sequencefile; 指定对应的分区中的表，并且重新设置其格式。 加载和导出数据从本地加载数据123load data local inpath '/home/hadopp/data.txt'overwrite into table employeespartition(country='US',state='CA'); 需要注意的是创建分区表的时候使用的是partition by。如果目录不存在，hive会先创建分区目录。 通过查询语句加载数据123insert overwrite table employeespartition(country='US',state='CA')select * from table_name where xxx=xxx; 通过查询语句建表12create table if not exists table_name as select * from table_name_b; 导出数据12345--方法一，谁用hadoop提供的工具hadoop fs -cp source_path target_path--方法二insert overwrite local directory '/home/hadoop/employees'select * from employees; hive的连接操作table stu 12341 chenli 212 xuzeng 223 xiaodan 234 hua 24 table sub 12341 chinese2 english3 science5 nature 内连接inner join，关键字 join on。仅列出两个表中符合连接条件的数据。 12345select a.*,b.* from stu a join sub b on a.id=b.id--结果1 chenli 21 1 chinese2 xuzeng 22 2 english3 xiaodan 23 3 science join后面连接表，而on指定连接条件。 左连接和右连接左连接，显示左边表的所有数据，如果右边表有与之对应的数据则显示，否则显示为NULL。 123456select a.* from stu a left outer join sub b on a.id=b.id;--结果1 chenli 21 1 chinese2 xuzeng 22 2 english3 xiaodan 23 3 science4 hua 24 NULL NULL 右连接与左连接相反，使用的关键字为 right outer join xxx on xxx。 标准查询关键字执行顺序为 from-&gt;where-&gt;group by-&gt;having-&gt;order by。 全连接左表和右表都显示，如果没有对应数据，则都显示为NULL 1234567select a.* from stu a full outer join sub b on a.id=b.id;--结果1 chenli 21 1 chinese2 xuzeng 22 2 english3 xiaodan 23 3 science4 hua 24 NULL NULLNULL NULL NULL 5 nature 左半开连接左半开连接。left semi join，语法与左连接不一样，只能选择出左边表的数据，此数据符合on后面的条件。 1234567select a.* from stu a left semi join sub b on a.id=b.id;--结果1 chenli 212 xuzeng 223 xiaodan 23--下列语句会报错select a.*,b.* from stu a left semi join sub b on a.id=b.id; 笛卡尔连接123456789101112131415161718select a.*,b.* from cl_student a join cl_stu_sub b;--结果1 chenli 21 1 chinese1 chenli 21 2 english1 chenli 21 3 science1 chenli 21 5 nature2 xuzeng 22 1 chinese2 xuzeng 22 2 english2 xuzeng 22 3 science2 xuzeng 22 5 nature3 xiaodan 23 1 chinese3 xiaodan 23 2 english3 xiaodan 23 3 science3 xiaodan 23 5 nature4 hua 24 1 chinese4 hua 24 2 english4 hua 24 3 science4 hua 24 5 nature 花了几天的时间整理了hive的用法，终于不用在对着SQL摸瞎了，加油吧进击的SQL boy！ 日常福利(●´∀｀●)]]></content>
      <categories>
        <category>大数据</category>
        <category>hive编程</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive数据类型]]></title>
    <url>%2F2019%2F09%2F08%2Fhive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[文本文件数据编码 TSV：tab separated values；即“制表符分隔值”，用制表符分隔数据 CSV： comma separated values；即“逗号分隔值”，用逗号分隔数据 两种文件存在的缺点在于文件中可能存在不需要作为分隔符的逗号或者制表符存在，所有hive有专门的分隔符。hive记录中默认的分隔符 分隔符 描述 \n 对于文本文件来说每一行都是记录，可以使用换行符作为分隔符 ^A(ctrl+A) 用于分隔字段(列)，在CREATE TABLE 语句中可以使用八进制编码\001表示，键盘上打不出来。 ^B 用于分隔array或者struct中的元素，或于用map钟键值对的分隔，在CREATE TABLE中使用\002表示 ^C 用于MAP钟键与值的分隔，用\003表示 读时模式传统数据库是写时模式，即在写入文件的时候会，会对数据的格式进行校验，如果不符合，将无法写入数据库。 hive是读时模式，在往数据库里写入不会对数据进行校验，但是在读取数据的时候会对数据进行校验，对于不合格的数据，会设置为null。 hive的优点在于加载(写)数据的时候速度较快，因为不需要对数据进行解析，而传统写时模式则有利于数据的查询。 好久没有更新博客了，这篇虽然水了点，写得像个笔记，算是开篇吧，福利就上亚丝娜吧!!!]]></content>
      <categories>
        <category>大数据</category>
        <category>hive编程</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>编码类型</tag>
      </tags>
  </entry>
</search>
