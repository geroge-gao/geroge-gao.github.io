<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">
  <link rel="alternate" href="/atom.xml" title="心如止水" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":2,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="一问搞懂词向量词向量将数据从高维空间向低维空间映射，虽然降维了，但是包含语意信息word2vec两种网络结构： CBOW：输入$w_t$周边，得到$w_t$ Skip-gram：输入$w_t$，得到其周边 优化方式 负采样 层次softmax FastTextTextCNNtransfromer论文笔记">
<meta name="keywords" content="推荐系统,Embedding,词向量,自然语言处理">
<meta property="og:type" content="article">
<meta property="og:title" content="一文搞懂词向量">
<meta property="og:url" content="http://yoursite.com/2021/12/26/一文搞懂词向量/index.html">
<meta property="og:site_name" content="心如止水">
<meta property="og:description" content="一问搞懂词向量词向量将数据从高维空间向低维空间映射，虽然降维了，但是包含语意信息word2vec两种网络结构： CBOW：输入$w_t$周边，得到$w_t$ Skip-gram：输入$w_t$，得到其周边 优化方式 负采样 层次softmax FastTextTextCNNtransfromer论文笔记">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/transformer/transformer%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/transformer/Multi-Head_Attention.png">
<meta property="og:image" content="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/bert/bert%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/bert/bert%E8%BE%93%E5%85%A5%E5%90%91%E9%87%8F.png">
<meta property="og:updated_time" content="2021-12-26T15:39:16.771Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="一文搞懂词向量">
<meta name="twitter:description" content="一问搞懂词向量词向量将数据从高维空间向低维空间映射，虽然降维了，但是包含语意信息word2vec两种网络结构： CBOW：输入$w_t$周边，得到$w_t$ Skip-gram：输入$w_t$，得到其周边 优化方式 负采样 层次softmax FastTextTextCNNtransfromer论文笔记">
<meta name="twitter:image" content="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/transformer/transformer%E7%BB%93%E6%9E%84.png">
  <link rel="canonical" href="http://yoursite.com/2021/12/26/一文搞懂词向量/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>一文搞懂词向量 | 心如止水</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">心如止水</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/12/26/一文搞懂词向量/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="gerogegao">
      <meta itemprop="description" content="世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活。">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="心如止水">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">一文搞懂词向量

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2021-12-26 23:38:05 / 修改时间：23:39:16" itemprop="dateCreated datePublished" datetime="2021-12-26T23:38:05+08:00">2021-12-26</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/推荐系统/" itemprop="url" rel="index"><span itemprop="name">推荐系统</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/推荐系统/Embedding/" itemprop="url" rel="index"><span itemprop="name">Embedding</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="一问搞懂词向量"><a href="#一问搞懂词向量" class="headerlink" title="一问搞懂词向量"></a>一问搞懂词向量</h1><h2 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h2><p>将数据从高维空间向低维空间映射，虽然降维了，但是包含语意信息</p><h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>两种网络结构：</p><ul>
<li>CBOW：输入$w_t$周边，得到$w_t$</li>
<li>Skip-gram：输入$w_t$，得到其周边</li>
</ul><p>优化方式</p><ul>
<li>负采样</li>
<li>层次softmax</li>
</ul><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><h2 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h2><h2 id="transfromer论文笔记"><a href="#transfromer论文笔记" class="headerlink" title="transfromer论文笔记"></a>transfromer论文笔记</h2><a id="more"></a>




<p>transformer提出的目的：</p>
<ol>
<li>解决rnn/lstm等序列过长时出现的长期依赖问题</li>
<li>解决序列模型并行化的问题，rnn类模型当前$t$时刻的计算需要依赖$t-1$时刻的计算结果</li>
</ol>
<p>Transformer做出的贡献：</p>
<ol>
<li>抛除了CNN/RNN，完全采用注意力机制，增加了并行度</li>
<li>采用position encode，将序列距离缩放到常量</li>
<li>在nlp任务重能达到当前最先进的</li>
</ol>
<p>网络结构</p>
<p><img src="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/transformer/transformer%E7%BB%93%E6%9E%84.png" alt="image-20211205163557052"></p>
<p>transformer整体采用的是一个encoder-decoder架构</p>
<p>关于KQV讲得不错的一篇博客</p>
<p><a href="https://zhuanlan.zhihu.com/p/410776234" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/410776234</a></p>
<p>Encoder和Decoder层</p>
<p><strong>Encoder</strong></p>
<p>Encoder包含6层网络，每一层网络有两层子网络。第一层是多头注意力机制，第二个子层是简单神经网络。</p>
<p>对于一层网络的两个子层之间，采用残差网络来连接。每一个子层之间都是用layer normalization，所以每个子层的输出为</p>
<script type="math/tex; mode=display">
LayerNorm(x+SubLayer(x))</script><p>并且为了实现残差连接，每个子层的embedding输出的维度$d_{model}$都相等，为512维</p>
<p><strong>Decoder</strong></p>
<p>和encoder一样，由6层网络组成，每层网络比encoder多了一层mask multi-head attention layer的子层，防止编码位置泄露，使得预测的每一个位置都依赖的是前面已知的输出，其实就是防止数据泄露。</p>
<p><a href="https://ifwind.github.io/2021/08/17/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89Mask%E6%9C%BA%E5%88%B6/#unilm%E4%B8%AD%E7%9A%84mask" target="_blank" rel="noopener">nlp常用mask操作</a></p>
<p><strong>Attention</strong></p>
<p>注意力机制，其实就是query和value向量映射到一个输出向量，输出向量作为value的加权和。</p>
<p>Scaled Dot-Product Attention</p>
<p>其实就是一个普通的attention归一化了。</p>
<p>Attention的输出为</p>
<script type="math/tex; mode=display">
Attention = softmax(\frac{QK^T}{\sqrt{d_k}})\dot V</script><p>国外一篇博客的翻译</p>
<p><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48508221</a></p>
<p>解释了使用Dot Attention和归一化的原因</p>
<ul>
<li>使用Dot-product attention而不是Addive attention的原因是在实际实现当中，Dot Attention更快并且空间效率更高。</li>
<li>当$d_k$的值比较小的时候，两种注意力机制结果差不多。$d_k$的值比较大的时候，加法表现得更好。因此论文假设当$d_k$维度比较大的时候，点乘的结果会变得很大，导致softmax的函数出现梯度消失的问题。为了克服这个影响，进行了归一化。</li>
</ul>
<p>Multi-Head Attention</p>
<p><img src="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/transformer/Multi-Head_Attention.png" alt="image-20211206004115066"></p>
<p>multi-head其实也很好理解，其实就是将一个self-attention映射了多次，然后将其结果拼接起来。这么做的好处是可以将不同位置的不同表达子空间关联起来。</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head_1, ..., head_n)W^o
\newline
where head_i=Attention(QW^Q_i, KW^K_i,VW^V_i)</script><p>论文中说明了h=8，$d_k=d_v=d_{model}/h=64$.</p>
<p>论文总结了在transformer中使用注意力机制的三个不同的方式</p>
<ul>
<li><p><strong>Position-wise Feed-Forward Networks</strong></p>
</li>
</ul>
<p>FNN的计算公式为</p>
<script type="math/tex; mode=display">
FFN(x)=max(0,xW_1+b_1)W_2+b_2</script><p><strong>Embeddings and Softmax</strong></p>
<p>暂时没看出来写了个啥</p>
<p><strong>Positional Encoding</strong></p>
<p>由于没有RNN或者卷积，所以为了使用序列的顺序，transformer在输入的embedding底部都加了position encoding，positional encoding和embedding有相同的维度$d_{model}$，方便两者相加。</p>
<p>Positional encoding有两种计算方式， sin和cos</p>
<script type="math/tex; mode=display">
PE(pos,2i)=sin(pos/10000^{2i/d_{model}})
\newline
PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}})</script><p>pos表示当前词的位置，i表示的是纬度，所以假设某个词的位置编码为pos=1，那么对应的编码为</p>
<script type="math/tex; mode=display">
[sin(1/10000^{0/512}), cos(1/10000^{2/512}), sin(1/10000^{4/512}.....]</script><p>采用positional encoding的原因</p>
<ul>
<li>需要体现一个单词在不同位置的区别</li>
<li>体现先后次序，一定范围内编码差异不依赖长度</li>
<li>能够将编码的值缩放到一定的范围</li>
</ul>
<p><a href="https://www.zhihu.com/question/347678607" target="_blank" rel="noopener">https://www.zhihu.com/question/347678607</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44121378</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48508221</a></p>
<h2 id="bert"><a href="#bert" class="headerlink" title="bert"></a>bert</h2><p>主要贡献：</p>
<ul>
<li>提出了双向预训练模型的表示，使用MLM 语言模型能够预训练双向深度表示。因为对于语言模型来说，一个词的意思不仅和左边有关系，可能和右边的句子也存在着关系。</li>
<li>证明了预训练表示能够降低对于特定任务专门设计架构的需求</li>
<li>在11项nlp任务达到当前领先的水平</li>
</ul>
<p>框架架构：</p>
<p>bert主要有两个步骤，一是pre-training，二是fine-tune</p>
<p>pre-training一般是在未标注数据上训练的，fine-tune则是在不同的模型上进行训练微调，bert最显著的特点就是统一了不同任务的结构，意思是bert具有通用性。</p>
<h3 id="bert模型结构"><a href="#bert模型结构" class="headerlink" title="bert模型结构"></a>bert模型结构</h3><p>bert是一个多层双向transformer组成的，在论文中提到了$Bert_{base}$和$Bert_{larg}$两个模型，其实就是参数不一致。</p>
<p><img src="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/bert/bert%E7%BB%93%E6%9E%84.png" alt=""></p>
<p>对比来说GPT采用的是单向transformer，ELmo采用是双向lstm，但是目标函数其实是不一样的，ELMo的目标函数分别为$P(w_i|w_1,w_2,…,w_{i-1})$和$P(w_i|w_{i+1},w_{i+2},…,w_{i+n})$。</p>
<h3 id="Bert输入表示"><a href="#Bert输入表示" class="headerlink" title="Bert输入表示"></a>Bert输入表示</h3><p><img src="https://raw.githubusercontent.com/geroge-gao/Images/master/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E8%AF%8D%E5%90%91%E9%87%8F/base_embedding/bert/bert%E8%BE%93%E5%85%A5%E5%90%91%E9%87%8F.png" alt="image-20211219170322265"></p>
<p>Bert输入表示分为三个embedding</p>
<ul>
<li>Token Embeddings：词向量embedding，对于每一句的开头，都有cls标识，最后隐藏层对应这个token的作为分类任务的表示</li>
<li>Segment Embeddings：判断当前的token到底是属于sentence A还是sentence B</li>
<li>Position Embeddings：每个词向量的位置embedding，不同于transformer，Bert的位置embedding是学出来的</li>
</ul>
<p>需要注意的一点，bert在处理句子对的时候，是将其打包起来，和单个句子一样处理，但是有sep作为分隔符。</p>
<p><a href="https://www.zhihu.com/question/374835153" target="_blank" rel="noopener">关于三个embedding的理解</a></p>
<h3 id="pre-training"><a href="#pre-training" class="headerlink" title="pre-training"></a>pre-training</h3><h4 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h4><p>masked language model，随机将句子中的某些词进行遮掩，然后预测被遮掩的词。在所有的Token当中随机选择15%的词用[mask]遮掩掉。但是不能一直使用[masked]作为遮掩，因为在fine-tune阶段，并不会碰到[mask]，所以为了减少影响，采用三种不同的策略。</p>
<ul>
<li>80%的token使用[mask]来遮掩</li>
<li>10%的token使用真实token代替</li>
<li>10%的token不进行遮掩操作。</li>
</ul>
<p>这样做的目的是使模型不知道哪个token被mask了，所以会关注每一个词。</p>
<h4 id="NSP"><a href="#NSP" class="headerlink" title="NSP"></a>NSP</h4><p>Next Sentence Prediction，对于不同的下游任务，例如问答系统或者自然语言推理是基于两个句子的关系，这没有被语言模型直接捕捉到，所以在建模的时候，输入句子A和句子B，B有一半的机率是A的下一句，50%的机率从句子中随机选择的，然后通过模型来预测A和B是否为下一句，模型准确率能达到97%。</p>
<h2 id="Fine-Tune"><a href="#Fine-Tune" class="headerlink" title="Fine-Tune"></a>Fine-Tune</h2><p>由于Transformer的attention机制，可以直接使用bert来处理下游的任务。对于包含文本对的输入/输出，其他方法是先分开编码然后再拼接。而bert直接拼接在一起使用双向self-attention，化繁为简。</p>
<p>对于分类任务，直接去[cls]最后输出层的final hidden state然后接入softmax做预测。文章也讲了下可调参数，其实也没啥，就batch size，learning_rate之类的。</p>
<p>参考资料:</p>
<p><a href="https://zhuanlan.zhihu.com/p/46652512" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46652512</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49271699</a></p>
<h2 id="Graph-embedding"><a href="#Graph-embedding" class="headerlink" title="Graph embedding"></a>Graph embedding</h2><h3 id="deepwalk"><a href="#deepwalk" class="headerlink" title="deepwalk"></a>deepwalk</h3><h3 id="node2vec"><a href="#node2vec" class="headerlink" title="node2vec"></a>node2vec</h3><h3 id="line"><a href="#line" class="headerlink" title="line"></a>line</h3><h3 id="eges"><a href="#eges" class="headerlink" title="eges"></a>eges</h3><p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49271699</a></p>
<p>nlp几大基本任务</p>
<p><a href="https://zhuanlan.zhihu.com/p/109122090" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/109122090</a></p>

    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.png" alt="gerogegao 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/推荐系统/" rel="tag"># 推荐系统</a>
            
              <a href="/tags/Embedding/" rel="tag"># Embedding</a>
            
              <a href="/tags/词向量/" rel="tag"># 词向量</a>
            
              <a href="/tags/自然语言处理/" rel="tag"># 自然语言处理</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2021/10/30/spark配置第三方jar包/" rel="next" title="spark配置第三方jar包">
                  <i class="fa fa-chevron-left"></i> spark配置第三方jar包
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一问搞懂词向量"><span class="nav-number">1.</span> <span class="nav-text">一问搞懂词向量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#词向量"><span class="nav-number">1.1.</span> <span class="nav-text">词向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#word2vec"><span class="nav-number">1.2.</span> <span class="nav-text">word2vec</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FastText"><span class="nav-number">1.3.</span> <span class="nav-text">FastText</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TextCNN"><span class="nav-number">1.4.</span> <span class="nav-text">TextCNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transfromer论文笔记"><span class="nav-number">1.5.</span> <span class="nav-text">transfromer论文笔记</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bert"><span class="nav-number">1.6.</span> <span class="nav-text">bert</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bert模型结构"><span class="nav-number">1.6.1.</span> <span class="nav-text">bert模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bert输入表示"><span class="nav-number">1.6.2.</span> <span class="nav-text">Bert输入表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pre-training"><span class="nav-number">1.6.3.</span> <span class="nav-text">pre-training</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MLM"><span class="nav-number">1.6.3.1.</span> <span class="nav-text">MLM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NSP"><span class="nav-number">1.6.3.2.</span> <span class="nav-text">NSP</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fine-Tune"><span class="nav-number">1.7.</span> <span class="nav-text">Fine-Tune</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-embedding"><span class="nav-number">1.8.</span> <span class="nav-text">Graph embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#deepwalk"><span class="nav-number">1.8.1.</span> <span class="nav-text">deepwalk</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#node2vec"><span class="nav-number">1.8.2.</span> <span class="nav-text">node2vec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#line"><span class="nav-number">1.8.3.</span> <span class="nav-text">line</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eges"><span class="nav-number">1.8.4.</span> <span class="nav-text">eges</span></a></li></ol></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avator.png"
      alt="gerogegao">
  <p class="site-author-name" itemprop="name">gerogegao</p>
  <div class="site-description" itemprop="description">世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活。</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">39</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/geroge-gao" title="GitHub &rarr; https://github.com/geroge-gao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="/gzj_1101@163.com" title="E-Mail &rarr; gzj_1101@163.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.zhihu.com/people/xiao-gao-2/activities" title="知乎 &rarr; https://www.zhihu.com/people/xiao-gao-2/activities" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>知乎</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">by gerogegao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.0</div>

        












        
      </div>
    </footer>
  </div>

  


    
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>



  





















  

  

  

  

</body>
</html>
